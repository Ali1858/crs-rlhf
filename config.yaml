default:
  debug_set: 100
  dtype: bf16
  report_to: "wandb"
  int8_training: false
  int4_training: true
  model_name: &sft_model meta-llama/Llama-2-7b-hf
  pre_sft: &pre_sft_name LLama-2-7b_pre_sft
  oasst_sft: &sft_adapter_name LLama-2-7b_crs_oasst_sft
  ranking_rm: &rm_ranking_name LLama-2-7b_crs_oasst_sft_reward
  rl_fine_tune: &rl_fine_tune LLama-2-7b_crs_oasst_sft_rl
  gradient_checkpointing: true
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon:
  log_steps: 10
  eval_steps: 100
  save_steps: 100
  checkpoint_number: final_checkpoint
  adapter_number: final_checkpoint
  lr_scheduler_type: cosine
  lr: 5e-5 #1e-4
  warmup_ratio: 0.03
  weight_decay: 0.0
  max_grad_norm: 0.3
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 1
  resume_from_checkpoint: 
  checkpoint_name:
  adpater_name:
  is_abs_rm:
  hpt_data_frac:
  init_from_adapter:
  early_stopping: false
  eval_accumulation_steps: 
  max_steps: 0
  merged_adapter_path:
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: False
    use_system_prefix: false
    system_prefix: null


pre_sft:
  output_dir: output/sft
  name: *pre_sft_name
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  int4_training: false
  gradient_accumulation_steps: 16
  eval_steps: 100
  save_steps: 300
  early_stopping: false
  dataset:
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200

sft:
  output_dir: output/sft
  name: *sft_adapter_name
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  int4_training: false
  gradient_accumulation_steps: 16
  eval_steps: 50
  save_steps: 100
  init_from_adapter: LLama-2-7b_pre_sft_bs64_ep_1_not_quant_pad_token
  dataset:
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
        top_k: 1

rm:
  output_dir: output/rm
  name: LLama-2-7b-oasst-baseline_reward #*rm_ranking_name
  base_model_name: andreaskoepf/llama2-7b-oasst-baseline #*sft_model
  # base_model_name: meta-llama/Llama-2-13b-hf
  is_abs_rm: false
  int4_training: false
  int8_training: true
  adpater_name: #output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1_not_quant_pad_token
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 32
  eval_steps: 25
  log_steps: 5
  save_steps: 400
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"
    # oasst_export_abs:
    #   val_split: 0.05
    #   lang: "en,es,de,fr"
    #   abs_oversample_threshold: 0.4
    #   label_weight:
    #     violence: 0
    #     creativity: 0
    #     helpfulness: 0
    #     humor: 0
    #     toxicity: 0
    #     quality: 1

rl:
  output_dir: output/rl
  name: LLama-2-7b-oasst-baseline_rl #*rl_fine_tune
  debug_set: 6
  # eval_sft: False
  # eval_data: data/eval_extra.json
  # peft_id: output/rl/LLama-2-7b-oasst-baseline_rl_bs16_kl_001_clip_04_512_max_token_with_pad_eos_lr_141e5
  # # output/rl/LLama-2-7b-oasst-baseline_rl_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5
  # # output/rl/LLama-2-7b-oasst-baseline_rl_abs_quality_rw_075_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5_logits #_adjusted 
  # # output/rl/LLama-2-7b-oasst-baseline_rl_bs16_kl_001_clip_04_512_max_token_with_pad_eos_lr_141e5 
  # output_json_prefix: eval_extra
  model_name: andreaskoepf/llama2-7b-oasst-baseline #output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1_not_quant_pad_token/merged 
  reward_model_name: andreaskoepf/llama2-7b-oasst-baseline #output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1_not_quant_pad_token/merged
  crs: true
  crs_weight: 0.25
  adafactor: False
  load_reward_type: #ranking #abs or ranking
  # abs_adapter_name: output/rm/LLama-2-7b-oasst-baseline_reward_abs_bs128_ep_1_8bit_logistic #output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_128_ep_1_logistic
  abs_adapter_name: output/rm/LLama-2-7b-oasst-baseline_reward_abs_bs128_ep_1_8bit_logistic_s_075_augment_and__under
  #output/rm/LLama-2-7b-oasst-baseline_reward_abs_bs128_ep_1_8bit_logistic_wgt_loss_075_strength_only_quality_eos_token
  #LLama-2-7b-oasst-baseline_reward_abs_bs128_ep_1_8bit_logistic_wgt_loss_075_strength_only_quality
  ranking_adapter_name: output/rm/LLama-2-7b-oasst-baseline_reward_ranking_bs64_ep_1_8bit_bf16_eos_token
  #LLama-2-7b-oasst-baseline_reward_ranking_bs64_ep_1_8bit_bf16 
  #LLama-2-7b_crs_oasst_sft_reward_ranking_bs_64_ep_1_sft_no_quantized
  seed: 999
  log_step: 5 # Step interval for logging.
  # eval_steps: 100
  save_steps: 40
  ppo_config:
    steps: 20000 # Number of steps.
    learning_rate: 1.41e-5 #1.4026e-5 # Learning rate for PPO. 1.0e-6 #7.41e-5 #
    log_with: "wandb" # Logging method.
    batch_size: 16 # Batch size.
    mini_batch_size: 2 # Mini-batch size.
    gradient_accumulation_steps: 8 # Gradient accumulation steps.
    early_stopping: False # Whether to use early stopping.
    # target_kl: 0.1 
    # Target KL divergence. Usefull for kl-coefficient. if kl is less than target kl then kl coefficient will also be less
    target_kl: 1 # i think this is only usefull when earl stopping.
    ppo_epochs: 4 # Number of epochs for PPO training.
    seed: 90 # Random seed.
    init_kl_coef: 0.02
    # init_kl_coef: 0.2 # Initial KL coefficient. Decide strength of KL penalty
    adap_kl_ctrl: True # Whether to adapt KL control.
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"

### Evaluation config
sft_eval:
  name: LLama-2-7b_eval
  init_from_adapter: LLama-2-7b_pre_sft_warm_20
  # adapter_number: checkpoint-600
  dataset:
    vicuna:
      val_split: 0.05
      max_val_set: 800
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
        # top_k: 1
    webgpt:
      val_split: 0.05
      max_val_set: 1000
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: False
    use_system_prefix: false
    system_prefix: 

  
eval_abs_rm:
  name: *rm_ranking_name
  base_model_name: *sft_model
  is_abs_rm: true
  adpater_name: LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default
  train_batch: 4 #1
  eval_batch: 4 #1
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 100
  save_steps: 200
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    # oasst_export:
    oasst_export_abs:
      val_split: 0.05
      lang: "en,es,de,fr"
      label_weight:
        non violence: 0
        creativity: 0
        helpfulness: 0
        humor: 0
        non toxicity: 0
        quality: 1


eval_ranking_rm:
  name: *rm_ranking_name
  base_model_name: *sft_model
  is_abs_rm: false
  adpater_name: LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default
  train_batch: 4 #1
  eval_batch: 4 #1
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 100
  save_steps: 200
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"
