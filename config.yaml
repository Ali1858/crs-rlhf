default:
  debug_set: 100
  dtype: bf16
  report_to: "wandb"
  int8_training: false
  int4_training: true
  output_dir: output
  model_name: &sft_model meta-llama/Llama-2-7b-hf
  pre_sft: &pre_sft_name LLama-2-7b_pre_sft
  oasst_sft: &sft_adapter_name LLama-2-7b_oasst_sft
  ranking_rm: &rm_ranking_name LLama-2-7b_ranking_reward
  gradient_checkpointing: true
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  log_steps: 1
  eval_steps: 100
  save_steps: 100
  checkpoint_number: final_checkpoint
  adapter_number: final_checkpoint
  lr_scheduler_type: cosine
  lr: 1e-5
  warmup_steps: 50
  weight_decay: 0.000001
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 1
  resume_from_checkpoint: 
  checkpoint_name:
  adpater_name:
  is_abs_rm:
  hpt_data_frac:
  init_from_adapter:
  early_stopping: false
  eval_accumulation_steps: 
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: False
    use_system_prefix: false
    system_prefix: null


pre_sft:
  name: *pre_sft_name
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 2
  gradient_accumulation_steps: 8
  eval_steps: 100
  save_steps: 300
  early_stopping: true
  dataset:
    vicuna:
      val_split: 0.05
      max_val_set: 800
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200
    webgpt:
      val_split: 0.05
      max_val_set: 1000
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk

sft:
  name: *sft_adapter_name
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 3
  gradient_accumulation_steps: 16
  eval_steps: 25
  save_steps: 100
  init_from_adapter: LLama-2-7b_pre_sft_4bit_lr_1e5_bs_64_adam_hf
  dataset:
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
        top_k: 1

rm:
  name: *rm_ranking_name
  base_model_name: *sft_model
  adpater_name: LLama-2-7b_oasst_sft_4bit_lr_1e5_bs_64_top_1_adam_torch
  train_batch: 1
  eval_batch: 1
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 25
  save_steps: 200
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"

sft_eval:
  name: LLama-2-7b_eval
  init_from_adapter: LLama-2-7b_oasst_sft_4bit_lr_1e5_bs_64_top_1_adam_torch
  # adapter_number: checkpoint-300
  dataset:
    vicuna:
      val_split: 0.05
      max_val_set: 800
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
        # top_k: 1
