default:
  debug_set: 100
  dtype: bf16
  report_to: "wandb"
  int8_training: false
  int4_training: true
  model_name: &sft_model meta-llama/Llama-2-7b-hf
  pre_sft: &pre_sft_name LLama-2-7b_pre_sft
  oasst_sft: &sft_adapter_name LLama-2-7b_crs_oasst_sft
  ranking_rm: &rm_ranking_name LLama-2-7b_crs_oasst_sft_reward
  rl_fine_tune: &rl_fine_tune LLama-2-7b_crs_oasst_sft_rl
  gradient_checkpointing: true
  adam_beta1:
  adam_beta2: 0.999
  adam_epsilon:
  log_steps: 10
  eval_steps: 100
  save_steps: 100
  checkpoint_number: final_checkpoint
  adapter_number: final_checkpoint
  lr_scheduler_type: cosine
  lr: 1e-4
  warmup_ratio: 0.03
  weight_decay: 0.0
  max_grad_norm: 0.3
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 1
  resume_from_checkpoint: 
  checkpoint_name:
  adpater_name:
  is_abs_rm:
  hpt_data_frac:
  init_from_adapter:
  early_stopping: false
  eval_accumulation_steps: 
  max_steps: 0
  merged_adapter_path:
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: False
    use_system_prefix: false
    system_prefix: null


pre_sft:
  output_dir: output/sft
  name: *pre_sft_name
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 100
  save_steps: 300
  early_stopping: false
  dataset:
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200

sft:
  output_dir: output/sft
  name: *sft_adapter_name
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 50
  save_steps: 100
  init_from_adapter: LLama-2-7b_pre_sft_bs64
  dataset:
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
        top_k: 1

rm:
  output_dir: output/rm
  name: *rm_ranking_name
  base_model_name: *sft_model
  # base_model_name: meta-llama/Llama-2-13b-hf
  is_abs_rm: true
  adpater_name: output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1
  train_batch: 4
  eval_batch: 4
  num_train_epochs: 1
  gradient_accumulation_steps: 32
  eval_steps: 25
  log_steps: 5
  save_steps: 400
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    # oasst_export:
    #   val_split: 0.05
    #   lang: "en,es,de,fr"
    oasst_export_abs:
      val_split: 0.05
      lang: "en,es,de,fr"
      abs_oversample_threshold: 0.4
      label_weight:
        violence: 0
        creativity: 0.15
        helpfulness: 0.35
        humor: 0
        toxicity: -0.1
        quality: 0.4

rl:
  output_dir: output/rl
  name: *rl_fine_tune
  debug_set: 6
  model_name: output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1
  reward_model_name: output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1
  crs: false
  crs_weight:
  adafactor: False
  load_reward_type: ranking #abs or ranking
  abs_adapter_name: output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_128_ep_1_logistic
  ranking_adapter_name: output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs_64_ep_1
  seed: 999
  log_step: 5 # Step interval for logging.
  # eval_steps: 100
  save_steps: 40
  ppo_config:
    steps: 20000 # Number of steps.
    learning_rate: 1.41e-5 #1.41e-5 # Learning rate for PPO.
    log_with: "wandb" # Logging method.
    batch_size: 32 # Batch size.
    mini_batch_size: 2 # Mini-batch size.
    gradient_accumulation_steps: 16 # Gradient accumulation steps.
    early_stopping: False # Whether to use early stopping.
    target_kl: 0.1 # Target KL divergence.
    ppo_epochs: 4 # Number of epochs for PPO training.
    seed: 90 # Random seed.
    init_kl_coef: 0.2 # Initial KL coefficient.
    adap_kl_ctrl: True # Whether to adapt KL control.
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"





### Evaluation config
sft_eval:
  name: LLama-2-7b_eval
  init_from_adapter: LLama-2-7b_pre_sft_warm_20
  # adapter_number: checkpoint-600
  dataset:
    vicuna:
      val_split: 0.05
      max_val_set: 800
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200
    oasst_export:
        val_split: 0.05
        max_val_set: 
        lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
        # top_k: 1
    webgpt:
      val_split: 0.05
      max_val_set: 1000
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: False
    use_system_prefix: false
    system_prefix: 

  
eval_abs_rm:
  name: *rm_ranking_name
  base_model_name: *sft_model
  is_abs_rm: true
  adpater_name: LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default
  train_batch: 4 #1
  eval_batch: 4 #1
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 100
  save_steps: 200
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    # oasst_export:
    oasst_export_abs:
      val_split: 0.05
      lang: "en,es,de,fr"
      label_weight:
        non violence: 0
        creativity: 0
        helpfulness: 0
        humor: 0
        non toxicity: 0
        quality: 1


eval_ranking_rm:
  name: *rm_ranking_name
  base_model_name: *sft_model
  is_abs_rm: false
  adpater_name: LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default
  train_batch: 4 #1
  eval_batch: 4 #1
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_steps: 100
  save_steps: 200
  metrics:
    - accuracy
  max_replies: 4
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"