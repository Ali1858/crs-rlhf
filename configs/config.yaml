common:
  debug: false
  debug_set: 100
  dtype: bf16
  report_to: "wandb"
  int8_training: true
  output_dir: output
  model_name: &sft_model meta-llama/Llama-2-7b-hf
  resume_from_checkpoint: 
  checkpoint_name:
  checkpoint_number: 
  hpt_data_frac: 
  peft_config:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: none
    inference_mode: false 
 
pre_sft:
  name: &pre_sft_name LLama-2-7b_pre_sft
  train_batch: 4
  eval_batch: 4
  lr: 1e-5
  num_train_epochs: 1
  gradient_accumulation_steps: 16
  eval_accumulation_steps: 
  log_steps: 25
  eval_steps: 100
  save_steps: 300
  lr_scheduler_type: cosine
  warmup_steps: 20
  weight_decay: 0.000001
  gradient_checkpointing: true
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  # checkpoint_name: Llama-2-7b_pre_sft
  # checkpoint_number: checkpoint-600
  peft_config_additional:
    target_modules: all
    task_type: CAUSAL_LM
    modules_to_save:
      - embed_tokens
      - lm_head
  dataset:
    vicuna:
      val_split: 0.05
      max_val_set: 800
    dolly:
      val_split: 0.05
      max_val_set: 300
    alpaca:
      val_split: 0.05
      max_val_set: 200
    math_instruction:
      val_split: 0.05
      max_val_set: 200
    webgpt:
         val_split: 0.05
         max_val_set: 1000
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: true
    use_system_prefix: false
    system_prefix: null

sft:
  name: &sft_adapter LLama-2-7b_oasst_sft
  train_batch: 2
  eval_batch: 2
  lr: 1e-5
  num_train_epochs: 3
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 
  log_steps: 100
  eval_steps: 100
  save_steps: 200
  lr_scheduler_type: cosine
  warmup_steps: 100
  weight_decay: 0.000001
  gradient_checkpointing: true
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-12
  checkpoint_name: *pre_sft_name
  peft_config_additional:
    target_modules: all
    task_type: CAUSAL_LM
    modules_to_save:
      - embed_tokens
      - lm_head
  dataset:
    oasst_export:
        val_split: 0.2
        max_val_set: 200
        lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: true
    use_system_prefix: false
    system_prefix: null

rm:
  name: LLama-2-7b_ranking_reward
  base_model_name: *sft_model
  adpater_name: *sft_adapter 
  train_batch: 1
  eval_batch: 1
  lr: 1e-5
  num_train_epochs: 3
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 1
  log_steps: 500
  eval_steps: 1000
  save_steps: 5000
  warmup_steps: 20
  lr_scheduler_type: cosine
  weight_decay: 0.0
  gradient_checkpointing: true
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-15
  metrics:
    - accuracy
  max_replies: 4
  peft_config_additional:
    target_modules:
    task_type: SEQ_CLS
  dataset:
    oasst_export:
      val_split: 0.05
      lang: "en,es,de,fr"
    anthropic:
      splits:
        - train
        - test
      max_val_set: 1000
    shp:
      splits:
        - train
        - validation
      max_val_set: 1000
    hellaswag:
      splits:
        - train
        - validation
      max_val_set: 1000
    webgpt:
         val_split: 0.05
         max_val_set: 1000
    # hf_summary_pairs:
    #      splits:
    #       -train
    #       - valid1
    #      max_val_set: 250
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: true
    use_system_prefix: false
    system_prefix: null
