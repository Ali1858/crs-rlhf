default:
  model_name: meta-llama/Llama-2-7b-hf
  #openlm-research/open_llama_7b
  train_batch: 1
  eval_batch: 1
  lr: 1e-5
  num_train_epochs: 3
  gradient_accumulation_steps: 1
  eval_accumulation_steps: 1
  log_steps: 500
  eval_steps: 1000
  save_steps: 5000
  warmup_steps: 20
  weight_decay: 0.0
  dtype: fp16
  gradient_checkpointing: true
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-15
  resume_from_checkpoint: null
  report_to: "wandb"
  metrics:
    - accuracy
  int8_training: true
  peft_config:
    r: 16
    lora_alpha: 32
    target_modules: all
    lora_dropout: 0.05
    bias: none
    inference_mode: false
    task_type: CAUSAL_LM
    modules_to_save:
      # - wte
      - embed_tokens
      - lm_head
  dataset:
    vicuna:
      val_split: 0.2
      max_val_set: 200
    dolly:
      val_split: 0.2
      max_val_set: 200
    alpaca:
      val_split: 0.2
      max_val_set: 200
    math_instruction:
      val_split: 0.2
      max_val_set: 200
    oasst_export:
      val_split: 0.2
      max_val_set: 200
      lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk
  collator:
    max_length: 2048
    random_offset_probability: 0.5
    label_masking: true
    samples_mixing: true
    use_system_prefix: false
    system_prefix: null
  debug: true
