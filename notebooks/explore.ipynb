{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/miniforge3/envs/crs-rlhf-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from oasst_data import ExportMessageNode, read_dataset_message_trees, read_message_trees, visit_threads_depth_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_name = \"OpenAssistant/oasst1\"\n",
    "tree_iter = read_dataset_message_trees(hf_dataset_name, split=\"train+validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "from torch import Generator\n",
    "\n",
    "manual_seed=90\n",
    "generator = Generator()\n",
    "generator.manual_seed(manual_seed)\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, data: list):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "\n",
    "lang_codes = [\"en\"]\n",
    "val_split = 0.2\n",
    "lang = \"en\",\n",
    "top_k = None\n",
    "        \n",
    "def get_data(mode=\"sft\"):\n",
    "    threads_per_tree = []\n",
    "    for tree in tree_iter:\n",
    "        if tree.tree_state != \"ready_for_export\" or not tree.prompt.review_result or tree.prompt.lang not in lang_codes:\n",
    "            continue\n",
    "\n",
    "        if mode in (\"sft\", \"rm\"):\n",
    "            if tree.tree_state != \"ready_for_export\":\n",
    "                continue\n",
    "        elif mode == \"rl\":\n",
    "            if tree.tree_state not in (\"ready_for_export\", \"prompt_lottery_waiting\"):\n",
    "                continue\n",
    "\n",
    "        # extract all threads up to last assistant reply\n",
    "        threads: list[list[ExportMessageNode]] = []\n",
    "            \n",
    "        def thread_filter(thread: list[ExportMessageNode]) -> bool:\n",
    "            if any(m.deleted or m.synthetic for m in thread):\n",
    "                return False\n",
    "\n",
    "            if top_k is not None:\n",
    "                for i, m in enumerate(thread):\n",
    "                    if m.role == \"assistant\":\n",
    "                        if m.rank is None:\n",
    "                            if i > 0 and len(thread[i - 1].replies) > 1:\n",
    "                                return False\n",
    "                        elif m.rank >= top_k:\n",
    "                            return False\n",
    "            return True\n",
    "\n",
    "\n",
    "        def leaf_filter(thread: list[ExportMessageNode]) -> bool:\n",
    "                if mode == \"sft\":\n",
    "                    # in SFT mode `not thread[-1].replies` finds nodes without children (leaves).\n",
    "                    # We are interested in those which are role='assistant' but some trees don't end on assistant nodes\n",
    "                    # but have prompter leaves .. we want to use those trees too .. e.g. remove the last prompter message(s)\n",
    "                    # so that they end with assistant. The `thread[-2].replies[0] == thread[-1]` check makes sure that only\n",
    "                    # the FIRST prompter reply is added .. e.g. the parent does not appear multiple times and we can use\n",
    "                    # pop() to remove superfluous prompter leaf node later.\n",
    "                    return (\n",
    "                        len(thread) > 1\n",
    "                        and not thread[-1].replies\n",
    "                        and (thread[-1].role == \"assistant\" or thread[-2].replies[0] == thread[-1])\n",
    "                        and thread_filter(thread)\n",
    "                    )\n",
    "                elif mode == \"rm\":\n",
    "                    # for reward models we use thread-fragments ending on prompter messages as prefix and\n",
    "                    # their (ranked) replies as possible continuations.\n",
    "                    if thread[-1].replies is None:\n",
    "                        return False\n",
    "                    return (\n",
    "                        thread[-1].role == \"prompter\"\n",
    "                        and len([r for r in thread[-1].replies if r.rank is not None]) > 1\n",
    "                        and thread_filter(thread)\n",
    "                    )\n",
    "                elif mode == \"rl\":\n",
    "                    # during rl we are interested in all possible prefixes ending in prompter messages\n",
    "                    return thread[-1].role == \"prompter\" and not any(m.deleted or m.synthetic for m in thread)\n",
    "\n",
    "                raise RuntimeError()\n",
    "\n",
    "        def process_thread(thread: list[ExportMessageNode]):\n",
    "                if mode == \"sft\":\n",
    "                    # ensure roles are strictly alternating between prompter and assistant\n",
    "                    assert all(m.role == \"prompter\" for m in thread[0::2]) and all(m.role == \"assistant\" for m in thread[1::2])\n",
    "                    conversation: list[[]] = [[m.text,m.role,m.get_label_value(\"quality\"),m.get_label_value(\"humor\"),\n",
    "                                             m.get_label_value(\"creativity\")]\n",
    "                                            for m in thread\n",
    "                                           ]\n",
    "                    return conversation\n",
    "                elif mode == \"rm\":\n",
    "                    prefix = [m.text for m in thread]\n",
    "                    replies = [r for r in thread[-1].replies if r.role == \"assistant\" and r.rank is not None]\n",
    "                    replies = sorted(replies, key=lambda r: r.rank)\n",
    "                    replies = [r.text for r in replies]\n",
    "                    return (prefix, replies)\n",
    "                elif mode == \"rl\":\n",
    "                    return ([m.text for m in thread],)\n",
    "\n",
    "                raise RuntimeError()\n",
    "\n",
    "        visit_threads_depth_first(tree.prompt, visitor=threads.append, predicate=leaf_filter)\n",
    "        if mode == \"sft\":\n",
    "            for t in threads:\n",
    "                if t[-1].role == \"prompter\":\n",
    "                    t.pop()\n",
    "        threads_per_tree.append(threads)\n",
    "        # split on tree basis, messages from same tree must not end up in different splits\n",
    "        trees = ListDataset(threads_per_tree)\n",
    "        splits = random_split(trees, lengths=[1.0 - val_split, val_split], generator=generator)\n",
    "\n",
    "        def flatten(ds: ListDataset) -> ListDataset:\n",
    "            return ListDataset([process_thread(thread) for tree_threads in ds for thread in tree_threads])\n",
    "\n",
    "        train = flatten(splits[0])\n",
    "        val = flatten(splits[1])\n",
    "        return train,val,threads_per_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/miniforge3/envs/crs-rlhf-env/lib/python3.10/site-packages/torch/utils/data/dataset.py:348: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "train_sft,_,tsft = get_data(mode=\"sft\")\n",
    "train_rm,_,trm = get_data(mode=\"rm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tsft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in threads_per_tree[0]:\n",
    "    print([[m.text[:100],m.role,m.get_label_value(\"quality\"),\n",
    "      m.get_label_value(\"humor\"),\n",
    "      m.get_label_value(\"creativity\")] for m in t])\n",
    "    print('======'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "hf_dataset_name = \"OpenAssistant/oasst1\"\n",
    "dataset = load_dataset(hf_dataset_name,split=\"train+validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)\n",
    "def convert_hf_message(row: dict) -> None:\n",
    "    emojis = row.get(\"emojis\")\n",
    "    if emojis:\n",
    "        row[\"emojis\"] = dict(zip(emojis[\"name\"], emojis[\"count\"]))\n",
    "    labels = row.get(\"labels\")\n",
    "    if labels:\n",
    "        row[\"labels\"] = {\n",
    "            name: {\"value\": value, \"count\": count}\n",
    "            for name, value, count in zip(labels[\"name\"], labels[\"value\"], labels[\"count\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict: dict = None\n",
    "parents: list = None\n",
    "trees = []\n",
    "for row in dataset:\n",
    "    convert_hf_message(row)\n",
    "    if row[\"parent_id\"] is None:\n",
    "        if tree_dict:\n",
    "            trees.append([row['role'],row['text']])\n",
    "\n",
    "        tree_dict = {\n",
    "            \"message_tree_id\": row[\"message_id\"],\n",
    "            \"tree_state\": row[\"tree_state\"],\n",
    "            \"prompt\": row,\n",
    "        }\n",
    "        parents = []\n",
    "    else:\n",
    "        while parents[-1][\"message_id\"] != row[\"parent_id\"]:\n",
    "            parents.pop()\n",
    "        parent = parents[-1]\n",
    "        if \"replies\" not in parent:\n",
    "            parent[\"replies\"] = []\n",
    "        parent[\"replies\"].append(row)\n",
    "\n",
    "    row.pop(\"message_tree_id\", None)\n",
    "    row.pop(\"tree_state\", None)\n",
    "    parents.append(row)\n",
    "    if tree_dict:\n",
    "        trees.append([row['role'],row['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "c = {}\n",
    "\n",
    "for config_file in Path('.').glob(\"**/*.yaml\"):\n",
    "        no_conf = False\n",
    "        with config_file.open(\"r\") as f:\n",
    "            c.update(yaml.safe_load(f)[\"defaults\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[\"datasets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in c[\"datasets\"]:\n",
    "    if type(k)==dict:\n",
    "        k = list(k.keys())[0]\n",
    "    if k in SUMMARIZATION_DATASETS:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in c[\"datasets\"]:\n",
    "    if type(k)==dict:\n",
    "        k = list(k.keys())[0]\n",
    "    if k in QA_DATASETS:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in c[\"datasets\"]:\n",
    "    if type(k)==dict:\n",
    "        k = list(k.keys())[0]\n",
    "    if k in INSTRUCTION_DATASETS:\n",
    "        print(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIZATION_DATASETS = [\n",
    "    \"xsum\",\n",
    "    \"cnn_dailymail\",\n",
    "    \"samsum\",\n",
    "    \"multi_news\",\n",
    "    \"scitldr\",\n",
    "    \"billsum\",\n",
    "    \"debate_sum\",\n",
    "    \"tldr_news\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FORMAT_MAPPING = {\n",
    "    \"squad_v2\": {\"index_fn\": 'index_squad_v2'},\n",
    "    \"ua_squad\": {\n",
    "        \"index_fn\": 'index_uasquad',\n",
    "        \"name\": \"FIdo-AI/ua-squad\",\n",
    "        \"params\": {\"field\": \"data\"},\n",
    "        \"no_val\": True,\n",
    "    },\n",
    "    \"trivia_qa_nocontext\": {\n",
    "        \"index_fn\": 'index_trivia_qa_nocontext',\n",
    "        \"name\": \"trivia_qa\",\n",
    "        \"params\": {\"name\": \"rc.nocontext\"},\n",
    "    },\n",
    "    \"trivia_qa_context\": {\"index_fn\": 'index_trivia_qa_context', \"name\": \"trivia_qa\", \"params\": {\"name\": \"rc\"}},\n",
    "    \"adversarial_qa\": {\n",
    "        \"index_fn\": 'index_adversarial_qa',\n",
    "        \"params\": {\"name\": \"adversarialQA\"},\n",
    "    },\n",
    "    \"gsm8k_hard\": {\"index_fn\": 'index_gsm_hard', \"name\": \"reasoning-machines/gsm-hard\", \"no_val\": True},\n",
    "    \"gsm8k\": {\"index_fn\": 'index_gsm8k', \"params\": {\"name\": \"main\"}, \"validation\": \"test\"},\n",
    "    \"wikihow\": {\"name\": \"b-mc2/wikihow_lists\", \"index_fn\": 'index_wikihow', \"no_val\": True},\n",
    "    \"essay_instruction\": {\n",
    "        \"name\": \"ChristophSchuhmann/essays-with-instructions\",\n",
    "        \"index_fn\": 'index_essay_instruction',\n",
    "        \"no_val\": True,\n",
    "    },\n",
    "    \"math_qa\": {\n",
    "        \"index_fn\": 'index_math_qa',\n",
    "    },\n",
    "    \"reddit_eli5\": {\"name\": \"eli5\", \"index_fn\": 'index_eli5', \"split_postfix\": \"_eli5\"},\n",
    "    \"reddit_askh\": {\"name\": \"eli5\", \"index_fn\": 'index_eli5', \"split_postfix\": \"_askh\"},\n",
    "    \"reddit_asks\": {\"name\": \"eli5\", \"index_fn\": 'index_eli5', \"split_postfix\": \"_asks\"},\n",
    "}\n",
    "\n",
    "INSTRUCTION_DATASETS = {\n",
    "    # Note humaneval_mbpp_codegen_qa returns a code string that we would want to at least wrap in ``` marks`\n",
    "    \"humaneval_mbpp_codegen_qa\": \"OllieStanley/humaneval-mbpp-codegen-qa\",\n",
    "    # Write unit tests to do task X\n",
    "    \"humaneval_mbpp_testgen_qa\": \"OllieStanley/humaneval-mbpp-testgen-qa\",\n",
    "    \"grade_school_math_instructions\": \"qwedsacf/grade-school-math-instructions\",\n",
    "    \"recipes\": \"dctanner/oa_recipes\",\n",
    "    \"ubuntu_dialogue_qa\": \"sedthh/ubuntu_dialogue_qa\",\n",
    "    \"cmu_wiki_qa\": \"sedthh/cmu_wiki_qa\",\n",
    "    \"youtube_subs_howto100m\": \"totuta/youtube_subs_howto100M\",\n",
    "    \"iapp_wiki_qa_squad\": \"wannaphong/iapp_wiki_qa_squad_oa\",\n",
    "    \"zhihu-kol\": \"wangrui6/zhihu-kol\",\n",
    "    \"minimath\": \"kentsui/minimath\",\n",
    "    \"oa_wiki_qa_bart_10000row\": \"michaelthwan/oa_wiki_qa_bart_10000row\",\n",
    "    \"oa_leet10k\": \"ehartford/oa_leet10k\",\n",
    "    \"poem_instructions\": \"checkai/instruction-poems\",\n",
    "    \"oa_stackexchange\": \"donfu/oa-stackexchange\",\n",
    "    \"tell_a_joke\": \"mikegarts/oa_tell_a_joke_20000\",\n",
    "    \"wizardlm_70k\": \"ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\",\n",
    "    \"megacode\": \"rombodawg/MegaCodeTraining112k\",\n",
    "    \"evol_instruct_code\": \"nickrosh/Evol-Instruct-Code-80k-v1\",\n",
    "}\n",
    "\n",
    "\n",
    "QA_DATASETS = list(DATASET_FORMAT_MAPPING.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_datasets.dataset_utils import load_sft_dataset\n",
    "from model.training_utils import get_sft_tokenizer\n",
    "from config import SFT_TRAINING_CONFIG,TOKENIZER_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 507/507 [00:00<00:00, 4.98MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 593/593 [00:00<00:00, 4.33MB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 534k/534k [00:01<00:00, 337kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 330/330 [00:00<00:00, 1.10MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 673M/673M [02:24<00:00, 4.67MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [02:24<00:00, 144.11s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 192.60it/s]\n",
      "Generating train split: 94145 examples [00:02, 31616.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vicuna training data: 47038\n",
      "Size of vicuna training data: 11760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 7.87M/7.87M [00:01<00:00, 4.20MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 320.81it/s]\n",
      "Generating train split: 100%|██████████| 15015/15015 [00:00<00:00, 613331.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dolly training data: 12000\n",
      "Size of dolly training data: 3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.06M/8.06M [00:02<00:00, 3.95MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 463.82it/s]\n",
      "Generating train split: 20022 examples [00:00, 360937.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of alpaca training data: 16016\n",
      "Size of alpaca training data: 4005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.55M/2.55M [00:00<00:00, 2.60MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 285.46it/s]\n",
      "Generating train split: 100%|██████████| 8792/8792 [00:00<00:00, 612034.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] 1 entries of Dataset({\n",
      "    features: ['INSTRUCTION', 'RESPONSE', 'SOURCE'],\n",
      "    num_rows: 8792\n",
      "}) were invalid.\n",
      "Size of math_instruction training data: 7032\n",
      "Size of math_instruction training data: 1759\n"
     ]
    }
   ],
   "source": [
    "tokenizer, eos_token= get_sft_tokenizer(SFT_TRAINING_CONFIG,TOKENIZER_CONFIG)\n",
    "train_ds , eval_ds = load_sft_dataset(eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82086"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 2\n",
      "<|prompter|>How to code based on a Figma design.</s>\n",
      "<|assistant|>Figma is a popular design tool that allows designers to create and share user interface (UI) designs for websites and applications. As a developer, you can use these designs as a blueprint to build the UI for your project. Here's a step-by-step guide on how to code based on a Figma design:\n",
      "\n",
      "1. Access the Figma design: Obtain the Figma design file or URL from the designer. You may need to sign up for a free account if you don't have one already.\n",
      "2. Familiarize yourself with the design: Review the design and understand the layout, components, and color schemes used. Pay close attention to typography, spacing, and any custom components.\n",
      "3. Export assets: Figma allows you to export images, icons, and other assets in various formats (e.g., PNG, SVG, JPEG). Export any necessary assets, and optimize them for the web to ensure fast loading times.\n",
      "4. Choose a development environment: Set up a development environment suitable for the project. This could include setting up a local development server, selecting a code ed</s>\n"
     ]
    }
   ],
   "source": [
    "tt = train_ds[100]\n",
    "print(f'len {len(tt)}')\n",
    "print('\\n'.join([t for t in tt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "isinstance(t, torch.utils.data.IterableDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 1024,\n",
       " 'random_offset_probability': None,\n",
       " 'label_masking': True,\n",
       " 'samples_mixing': True,\n",
       " 'use_system_prefix': False,\n",
       " 'system_prefix': None,\n",
       " 'vicuna': {'class': functools.partial(<class 'training_datasets.sft_dataset.Vicuna'>, input_max_length=1024),\n",
       "  'val_split': 0.2},\n",
       " 'dolly': {'class': training_datasets.sft_dataset.DatabrickDolly15k,\n",
       "  'val_split': 0.2},\n",
       " 'alpaca': {'class': training_datasets.sft_dataset.AlpacaBaseDataset,\n",
       "  'val_split': 0.2},\n",
       " 'math_instruction': {'class': training_datasets.sft_dataset.MathInstruction,\n",
       "  'val_split': 0.2},\n",
       " 'cache_dir': './cache',\n",
       " 'model_name': '',\n",
       " 'train_batch': 1,\n",
       " 'eval_batch': 1,\n",
       " 'lr': 1e-05,\n",
       " 'num_train_epochs': 3,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'eval_accumulation_steps': 1,\n",
       " 'log_steps': 500,\n",
       " 'eval_steps': 1000,\n",
       " 'save_steps': 5000,\n",
       " 'warmup_steps': 20,\n",
       " 'weight_decay': 0.0,\n",
       " 'dtype': 'fp16',\n",
       " 'gradient_checkpointing': True,\n",
       " 'adam_beta1': '',\n",
       " 'adam_beta2': '',\n",
       " 'adam_epsilon': '',\n",
       " 'resume_from_checkpoint': None,\n",
       " 'metrics': ['accuracy'],\n",
       " 'peft_config': {'r': 16,\n",
       "  'lora_alpha': 32,\n",
       "  'target_modules': 'all',\n",
       "  'lora_dropout': 0.05,\n",
       "  'bias': 'none',\n",
       "  'task_type': 'CAUSAL_LM',\n",
       "  'modules_to_save': ['wte', 'lm_head']}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import SFT_TRAINING_CONFIG, DIALOGUE_COLLATOR_CONFIG,SFT_DATASET_CONFIG\n",
    "\n",
    "dict(**DIALOGUE_COLLATOR_CONFIG,**SFT_DATASET_CONFIG,**SFT_TRAINING_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crs-rlhf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
