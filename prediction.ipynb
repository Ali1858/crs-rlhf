{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===loading the oasst_export_abs dataset===\n",
      "\n",
      "{'violence': 0, 'creativity': 0, 'helpfulness': 0, 'humor': 0, 'toxicity': 0, 'quality': 1}\n",
      "============================== total 774 has None value for atleast 1 label\n",
      "============================== total 0 has been skipped because reward is negative\n",
      "{'violence': 0, 'creativity': 0, 'helpfulness': 0, 'humor': 0, 'toxicity': 0, 'quality': 1}\n",
      "============================== total 32 has None value for atleast 1 label\n",
      "============================== total 0 has been skipped because reward is negative\n",
      "OASST HF dataset: len(train)=38852, len(val)=2041\n",
      "Size of oasst_export_abs training data: 38852\n",
      "Size of oasst_export_abs validation data: 2041\n",
      "============================== Total training dataset size is 38852...\n",
      "\t============================== Validation size for oasst_export_abs dataset size is 2041...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from utils import (print_yaml_config,init_or_resume_from, read_yaml)\n",
    "from model_training.training_utils import load_for_inference\n",
    "from constants import TOKENIZER_SEPECIAL_TOKENS\n",
    "from training_datasets.dataset_utils import load_rm_dataset\n",
    "\n",
    "config = {}\n",
    "conf = read_yaml('./config.yaml')\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"eval_ranking_rm\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"eval_ranking_rm\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_ranking = argparse.Namespace(**config)\n",
    "\n",
    "config = {}\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"eval_abs_rm\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"eval_abs_rm\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_abs = argparse.Namespace(**config)\n",
    " \n",
    "config_abs.dataset[\"oasst_export_abs\"][\"label_weight\"] = {\"violence\": 0, \"creativity\": 0, \"helpfulness\": 0, \"humor\": 0, \"toxicity\": 0,\"quality\": 1}\n",
    "config_abs.dataset[\"oasst_export_abs\"][\"abs_oversample_threshold\"]=None#0.4\n",
    "config_abs.dataset[\"oasst_export_abs\"][\"top_k\"]=None\n",
    "# v3_wgt = {\"violence\": 0, \"creativity\": 0.15, \"helpfulness\": 0.35, \"humor\": 0, \"toxicity\": -0.1,\"quality\": 0.4}\n",
    "# qlty = {\"violence\": 0, \"creativity\": 0, \"helpfulness\": 0, \"humor\": 0, \"toxicity\": 0,\"quality\": 1}\n",
    "# mean = {\"violence\": -0.167, \"creativity\": 0.167, \"helpfulness\": 0.167, \"humor\": 0.167, \"toxicity\": -0.167,\"quality\": 0.165}\n",
    "# v4_wgt = {\"violence\": -0.1, \"creativity\": 0.2, \"helpfulness\": 0.1, \"humor\": 0.167, \"toxicity\": -0.1,\"quality\": 0.3}\n",
    "# v2_wgt = {\"violence\": -0.05, \"creativity\": 0.15, \"helpfulness\": 0.25, \"humor\": 0.05, \"toxicity\": -0.1, \"quality\": 0.4\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# v3\n",
    "# non violence: 0\n",
    "# creativity: 0.15\n",
    "# helpfulness: 0.35\n",
    "# humor: 0\n",
    "# non toxicity: 0.1\n",
    "# quality: 0.4\n",
    "        \n",
    "# v2\n",
    "# non violence: 0.05\n",
    "# creativity: 0.15\n",
    "# helpfulness: 0.25\n",
    "# humor: 0.05\n",
    "# non toxicity: 0.1\n",
    "# quality: 0.4\n",
    "# only qualiyt\n",
    "# non violence: 0\n",
    "# creativity: 0\n",
    "# helpfulness: 0\n",
    "# humor: 0\n",
    "# non toxicity: 0\n",
    "# quality: 1\n",
    "# mean\n",
    "# non violence: 0.167\n",
    "# creativity: 0.167\n",
    "# helpfulness: 0.167\n",
    "# humor: 0.167\n",
    "# non toxicity: 0.167\n",
    "# quality: 0.165\n",
    "\n",
    "\n",
    "config_abs.dataset[\"oasst_export_abs\"][\"top_k\"]=None\n",
    "\n",
    "\"\"\"Quality: Since quality directly pertains to how good, informative, and comprehensive the answer is, it should have the highest weight.\n",
    "\n",
    "Weight: 0.4\n",
    "Helpfulness: An answer's primary goal, especially in this context, is to be helpful. If it's not helping the user understand or solve a problem, its value diminishes.\n",
    "\n",
    "Weight: 0.25\n",
    "Creativity: While creativity might not be as crucial as quality or helpfulness, a creative response can provide more insight or a fresh perspective.\n",
    "\n",
    "Weight: 0.15\n",
    "Toxicity: It's vital to ensure responses are not toxic. However, since the provided responses were largely non-toxic, this label might not need a very high weight. Still, it's essential to consider it.\n",
    "\n",
    "Weight: 0.1 (This would work in a negative fashion, where higher toxicity reduces the overall score.)\n",
    "Violence: Similar to toxicity, ensuring non-violence is crucial. Since the given responses didn't exhibit violence, this can have a smaller weight.\n",
    "\n",
    "Weight: 0.05 (Again, this would work negatively.)\n",
    "Humor: While humor can be a bonus, it's not necessarily a priority in this context. Therefore, it should have the lowest weight.\n",
    "\n",
    "Weight: 0.05\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Given the clarified requirements and the focus on ensuring that the scalar reward reflects the desired properties of helpfulness, harmlessness, quality, and entertainment, let's proceed.\n",
    "\n",
    "# Non-violence & Non-toxicity (After Subtracting from 1): Since higher values are now desirable, and the majority of the data is skewed toward these values, these metrics will dominate the scalar reward if given too much weight. While harmlessness is a priority, we don't want it overshadowing helpfulness.\n",
    "\n",
    "# Helpfulness: This is a core metric. The response must be helpful for a general-purpose chatbot, so this should carry a significant weight. However, we don't want a harmless but unhelpful response to get a high reward.\n",
    "\n",
    "# Quality: This essentially determines the coherence, relevance, and overall acceptability of the response. It's a fundamental metric.\n",
    "\n",
    "# Creativity & Humor: Both these metrics add to the entertaining aspect. Humor might not always be relevant in every context, so while it should have a weight, it shouldn't be too high. Creativity, on the other hand, could be desirable more often as it can provide unique and insightful responses.\n",
    "\n",
    "# Based on the above considerations, here's a suggested distribution:\n",
    "\n",
    "# Non-violence: 0.20\n",
    "# Non-toxicity: 0.20\n",
    "# Helpfulness: 0.30\n",
    "# Quality: 0.20\n",
    "# Creativity: 0.05\n",
    "# Humor: 0.05\n",
    "# This distribution sums up to 1. The logic here is:\n",
    "\n",
    "# Harmlessness (Non-violence and Non-toxicity) collectively gets a weight of 0.40. This ensures the chatbot doesn't produce harmful content.\n",
    "# Helpfulness is given a significant weight (0.30) so that utility is prioritized.\n",
    "# Quality is given an equal weight as harmlessness metrics because for a general-purpose chatbot, the quality of response matters.\n",
    "# Entertainment (Creativity and Humor) gets a collective weight of 0.10, ensuring the chatbot can be engaging but without compromising on the core objectives.\n",
    "# This distribution aims to balance the objectives laid out and mitigate the effects of the skewness in the data for non-violence and non-toxicity.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "train_e , eval_abs = load_rm_dataset(config_abs)\n",
    "\n",
    "# train_er , eval_ranking = load_rm_dataset(config_ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4104"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ss for ss in s if ss <= 0.4 ]) #38852/3354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = []\n",
    "\n",
    "for e in train_e:\n",
    "    prefix, reply, score = e\n",
    "    s.append(score)\n",
    "\n",
    "# for e in eval_abs[\"oasst_export_abs\"]:\n",
    "#     prefix, reply, score = e\n",
    "#     s.append(score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(s, bins=10, edgecolor=\"k\", alpha=0.7)\n",
    "plt.title(\"Distribution of Float Numbers\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 09:27:59,852] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa0df7ae93249c9af6c1e4d558de4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1/merged/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.score.original_module.weight', 'base_model.model.score.modules_to_save.rank_ep1.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_args = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    # \"quantization_config\": BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     ),\n",
    "    \"cache_dir\": 'cache',\n",
    "    \"device_map\": {\"\":1},\n",
    "}\n",
    "\n",
    "model_name = 'output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1/merged/'\n",
    "\n",
    "# abs_adapter_mean_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64/final_checkpoint\"\n",
    "# abs_adapter_wavg_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg/final_checkpoint\"\n",
    "# # abs_adapter2_wavg_l2_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_l2_1e-5/final_checkpoint\"\n",
    "# abs_adapter3_wavg_l2_no_sig_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_l2_1e-5_no_sigmoid/final_checkpoint\"\n",
    "# abs_adapter3_wavg_no_sig_os_at05_top_1_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_wgt_v2_no_sig_oversampled_at05_wgt_loss/final_checkpoint\"\n",
    "\n",
    "# abs_adapter4_wavg_no_sig_os_at05_wgt_loss_top_1_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_wgt_v3_no_sig_oversampled_at05_wgt_loss/final_checkpoint\"\n",
    "# abs_adapter4_wavg_no_sig_os_at05_top_1_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_wgt_v3_no_sig_oversampled_at05/final_checkpoint\"\n",
    "\n",
    "\n",
    "# # abs_adapter4_no_sig_v2_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_v2_l2_1e-5_no_sigmoid/final_checkpoint\"\n",
    "# abs_adapter5_no_sig_v3_oversampled =\"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_v3_no_sigmoid_oversampled/final_checkpoint\"\n",
    "# abs_adapter5_no_sig_v3_d02 =\"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_v3_no_sigmoid_wgt_loss_115_d_0.2/final_checkpoint\"\n",
    "\n",
    "# # abs_adapter = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_test_replication/final_checkpoint\"\n",
    "# ranking_adapter_name= \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16/final_checkpoint\"\n",
    "# ranking_adapter_name2 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16_top_1/final_checkpoint\"\n",
    "\n",
    "\n",
    "# abs_mean = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_mean_oversampled/final_checkpoint\"\n",
    "# abs_v2_wgt = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v2_wgt_oversampled/final_checkpoint\"\n",
    "# abs_v3_wgt = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v3_wgt_oversampled/final_checkpoint\"\n",
    "# abs_only_quality = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_only_quality_oversampled/final_checkpoint\"\n",
    "# rank_top_1 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16_top_1/final_checkpoint\"\n",
    "# abs_v32_wgt_loss = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v32_wgt_oversampled_wgt_loss_0.75_capacity/final_checkpoint\"\n",
    "# abs_v32_wgt = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v32_wgt_oversampled/final_checkpoint\"\n",
    "# abs_v32_wgt_ep1 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_64_ep_1/final_checkpoint\"\n",
    "\n",
    "abs_adapter_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_64_ep_1/final_checkpoint\"\n",
    "abs_adapter_name2 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_128_ep_1_logistic/final_checkpoint\"\n",
    "abs_adapter_name3 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_128_ep_1_logistic_wgt_loss/final_checkpoint\"\n",
    "\n",
    "ranking_adapter_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16_top_1/final_checkpoint\"\n",
    "ranking_adapter_name2 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs_64_ep_1/final_checkpoint\"\n",
    "\n",
    "# Since reward models are trained using the same base model, we should use same model\n",
    "base_reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=1,**model_args\n",
    "    )\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, cache_dir='cache')\n",
    "\n",
    "\n",
    "base_reward_model = PeftModel.from_pretrained(\n",
    "    base_reward_model,\n",
    "    ranking_adapter_name2,\n",
    "    adapter_name=\"rank_ep1\",\n",
    "    is_trainable=False\n",
    "    )\n",
    "# base_reward_model.load_adapter(ranking_adapter_name2,adapter_name=\"rank_ep1\",is_trainable=False)\n",
    "\n",
    "# base_reward_model.load_adapter(abs_adapter_name,adapter_name=\"abs\",is_trainable=False)\n",
    "base_reward_model.load_adapter(abs_adapter_name2,adapter_name=\"abs_logistic\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_adapter_name3,adapter_name=\"abs_logistic_wgt\",is_trainable=False)\n",
    "\n",
    "# base_reward_model.load_adapter(abs_v2_wgt,adapter_name=\"abs_v2_wgt\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v3_wgt,adapter_name=\"abs_v3_wgt\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_only_quality,adapter_name=\"abs_only_quality\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(rank_top_1,adapter_name=\"rank_top_1\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v32_wgt_loss,adapter_name=\"abs_v32_wgt_loss\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v32_wgt,adapter_name=\"abs_v32_wgt\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v32_wgt_ep1,adapter_name=\"abs_v32_wgt_ep1\",is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32016, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): Linear(\n",
       "                in_features=11008, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (rank_ep1): Linear(in_features=4096, out_features=1, bias=False)\n",
       "          (abs_logistic): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_datasets.collators import AbsoluteScoreDataCollator, RankingDataCollator\n",
    "\n",
    "\n",
    "abs_collate_fn = AbsoluteScoreDataCollator(\n",
    "    tokenizer,\n",
    "    max_length=2048,\n",
    "    pad_to_multiple_of=16,\n",
    ")\n",
    "\n",
    "ranking_collate_fn = RankingDataCollator(\n",
    "    tokenizer,\n",
    "    max_length=2048,\n",
    "    pad_to_multiple_of=16,\n",
    "    max_replies=4#config_ranking.max_replies\n",
    ")\n",
    "\n",
    "# eval_abs,eval_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abs_logistic'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_reward_model.active_adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = eval_abs[\"oasst_export_abs\"]\n",
    "\n",
    "l_50 = []\n",
    "p_50 = []\n",
    "\n",
    "l = []\n",
    "p = []\n",
    "\n",
    "base_reward_model.set_adapter(\"abs_logistic\")\n",
    "for d in data:\n",
    "    inputs = abs_collate_fn([d])\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    inputs = inputs.to(base_reward_model.device)\n",
    "    logits = base_reward_model(input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                use_cache=False,\n",
    "                ).logits\n",
    "    if labels <= 0.4:\n",
    "        l_50.append(labels)\n",
    "        p_50.append(logits[0])\n",
    "    else:\n",
    "        l.append(labels)\n",
    "        p.append(logits[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1000, 0.3875, 0.1594, 0.2708, 0.3083, 0.3458], dtype=torch.float64),\n",
       " tensor([-1.1328, -0.1260, -1.4531, -1.0703, -0.5039, -1.6328],\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([0.2432, 0.4688, 0.1895, 0.2559, 0.3770, 0.1631], dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "l_50[54:60], p_50[54:60],sigmoid(p_50[54:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def abs_reward_metrics(predictions,labels):\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = [p.float() if p.dtype == torch.bfloat16 else p for p in predictions]\n",
    "    labels = [l.float() if l.dtype == torch.bfloat16 else l for l in labels]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'mse': mean_squared_error(labels, predictions),\n",
    "        'mae': mean_absolute_error(labels, predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.025611386407787512, 'mae': 0.12173174989692846}\n",
      "{'mse': 0.05307156072948854, 'mae': 0.1999579205688052}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1344905/2652289913.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l_50 = torch.tensor(l_50)\n",
      "/tmp/ipykernel_1344905/2652289913.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_50 = torch.tensor(p_50)\n",
      "/tmp/ipykernel_1344905/2652289913.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  l = torch.tensor(l)\n",
      "/tmp/ipykernel_1344905/2652289913.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p = torch.tensor(p)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "l_50 = torch.tensor(l_50)\n",
    "p_50 = torch.tensor(p_50)\n",
    "\n",
    "\n",
    "l = torch.tensor(l)\n",
    "p = torch.tensor(p)\n",
    "\n",
    "print(abs_reward_metrics(p_50,l_50))\n",
    "print(abs_reward_metrics(p,l))\n",
    "\n",
    "## from for bit logitstic sigmoid + wgt loss\n",
    "# {'mse': 0.04753870369349233, 'mae': 0.17926979778653887}\n",
    "# {'mse': 0.027010542591441575, 'mae': 0.13228511980761412}\n",
    "\n",
    "## from 4 bit logistic sigmoid\n",
    "# {'mse': 0.05205766017335934, 'mae': 0.19300111204519685}\n",
    "#  {'mse': 0.02084478758192499, 'mae': 0.11781447598714637}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(385, 1493)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "def get_count(label,pred):\n",
    "    pred = sigmoid(pred)\n",
    "    c = 0\n",
    "    for l,p in zip(label,pred):\n",
    "        # p = sigmoid(p)\n",
    "        if p < 0 or p > 1:\n",
    "            # print(p,l)\n",
    "            c +=1\n",
    "    print(c)\n",
    "\n",
    "get_count(l_50,p_50),get_count(l,p)\n",
    "\n",
    "len(p_50), len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alikhan/miniconda3/envs/torch_p310_ppo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:tensor([0.6333], dtype=torch.float64)\n",
      "******************************\n",
      "<s><|prompter|> Could you help\n",
      "====================rank_ep1\n",
      "sigmoid: tensor([0.5469], device='cuda:1', dtype=torch.bfloat16),original: tensor([0.1816], device='cuda:1', dtype=torch.bfloat16)\n",
      "==============================\n",
      "====================abs_logistic\n",
      "sigmoid: tensor([0.3965], device='cuda:1', dtype=torch.bfloat16),original: tensor([-0.4180], device='cuda:1', dtype=torch.bfloat16)\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "def get_reward(inputs,adapter_name):\n",
    "        base_reward_model.eval()\n",
    "        print(f\"{'=='*10}{adapter_name}\")\n",
    "        base_reward_model.set_adapter(adapter_name)\n",
    "        logits = base_reward_model(input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                use_cache=False,\n",
    "                ).logits\n",
    "        s = sigmoid(logits)\n",
    "        for idx in range(logits.shape[0]):\n",
    "                print(f'sigmoid: {s[idx]},original: {logits[idx]}')\n",
    "        print('==='*10)\n",
    "\n",
    "idx = 55\n",
    "\n",
    "data = eval_abs[\"oasst_export_abs\"][idx]\n",
    "inputs = abs_collate_fn([data])\n",
    "labels = inputs.pop(\"labels\")\n",
    "print(f'label:{labels}')\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# data = [[\"Hi how are you?\"],[\"I am good you piece of shit\",\"Giberish, Giberish saying Giberish, tell Giberish is not Giberish. Why are Giberish you Giberish\",\"I am good\",\"I am good, how are you doing? Please tell me how can I help you? \"]] \n",
    "\n",
    "# ====================rank_ep1\n",
    "# sigmoid: tensor([0.0850], dtype=torch.bfloat16),original: tensor([-2.3750], dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.0396], dtype=torch.bfloat16),original: tensor([-3.1875], dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.1396], dtype=torch.bfloat16),original: tensor([-1.8203], dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.3535], dtype=torch.bfloat16),original: tensor([-0.6055], dtype=torch.bfloat16)\n",
    "# ==============================\n",
    "# ====================abs_logistic\n",
    "# sigmoid: tensor([0.2715], dtype=torch.bfloat16),original: tensor([-0.9844], dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.2637], dtype=torch.bfloat16),original: tensor([-1.0234], dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.3555], dtype=torch.bfloat16),original: tensor([-0.5977], dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.4980], dtype=torch.bfloat16),original: tensor([-0.0040], dtype=torch.bfloat16)\n",
    "\n",
    "# data = eval_ranking[\"oasst_export\"][idx]\n",
    "# inputs, _ = ranking_collate_fn([data])\n",
    "inputs = inputs.to(base_reward_model.device)\n",
    "# print(inputs)\n",
    "print('***'*10)\n",
    "for d in tokenizer.batch_decode(inputs.input_ids):\n",
    "        print(f'{d[:30]}')\n",
    "\n",
    "for adp_name in [\"rank_ep1\",\"abs_logistic\"]:\n",
    "    get_reward(inputs,adp_name)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|prompter|> Could you help me solve a physics problem? An axis of length L and mass M is balanced on the side of the table at its center of gravity. A ball with a relative height h is dropped onto its most protruding end. What is the speed of the ball immediately after impact?</s><s><|assistant|> Sure! Given the brief question I will simplify the calculation by assuming that there is no energy conversion from sources like heat dissipation, friction or noise. With that we can assume that the ball and the axis perform an elastic collision since both can move freely with the added complication that the axis is balanced on the table.\n",
      "\n",
      "First we need to calculate the balls velocity at the time of impact (v1):\n",
      "\n",
      "g = earth gravity 9.81m/s^2\n",
      "v1 = sqrt(2*g*h)\n",
      "\n",
      "Since the axis is balanced at the table edge it acts like a class 1 lever potentially resulting in a mechanical advantage.\n",
      "Due to the balance in the center the mechanical advantage of that lever is 1 since both sides are equally long.\n",
      "With an advantage of one we can just ignore the levering action and calculate a fully elastic collision.\n",
      "\n",
      "v2 being the velocity after impact\n",
      "mb being the mass of the ball\n",
      "ma being the mass of the axis\n",
      "\n",
      "v2 = (mb - ma) / (mb + ma) * v1\n",
      "\n",
      "combined with the first equation the result is:\n",
      "\n",
      "v = (mb - ma) / (mb + ma) * sqrt(2 * g * h)</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "====================rank_ep1\n",
      "sigmoid: tensor([0.5469], device='cuda:1', dtype=torch.bfloat16),original: tensor([0.1816], device='cuda:1', dtype=torch.bfloat16)\n",
      "==============================\n",
      "====================abs_logistic\n",
      "sigmoid: tensor([0.3965], device='cuda:1', dtype=torch.bfloat16),original: tensor([-0.4180], device='cuda:1', dtype=torch.bfloat16)\n",
      "==============================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================rank_ep1\n",
    "# sigmoid: tensor([0.5469], device='cuda:1', dtype=torch.bfloat16),original: tensor([0.1816], device='cuda:1', dtype=torch.bfloat16)\n",
    "# ==============================\n",
    "# ====================abs_logistic\n",
    "# sigmoid: tensor([0.3965], device='cuda:1', dtype=torch.bfloat16),original: tensor([-0.4180], device='cuda:1', dtype=torch.bfloat16)\n",
    "# ==============================\n",
    "\n",
    "# ====================rank_ep1\n",
    "# sigmoid: tensor([0.5391], device='cuda:1', dtype=torch.bfloat16),original: tensor([0.1494], device='cuda:1', dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.0216], device='cuda:1', dtype=torch.bfloat16),original: tensor([-3.8125], device='cuda:1', dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.0542], device='cuda:1', dtype=torch.bfloat16),original: tensor([-2.8594], device='cuda:1', dtype=torch.bfloat16)\n",
    "# ==============================\n",
    "# ====================abs_logistic\n",
    "# sigmoid: tensor([0.3984], device='cuda:1', dtype=torch.bfloat16),original: tensor([-0.4141], device='cuda:1', dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.1572], device='cuda:1', dtype=torch.bfloat16),original: tensor([-1.6797], device='cuda:1', dtype=torch.bfloat16)\n",
    "# sigmoid: tensor([0.1582], device='cuda:1', dtype=torch.bfloat16),original: tensor([-1.6719], device='cuda:1', dtype=torch.bfloat16)\n",
    "# ==============================\n",
    "\n",
    "\n",
    "d = [['Hi how are you?'],'I am good you piece of shit',0]\n",
    "d2 = [['Hi how are you?'],\"Giberish, Giberish saying Giberish, tell Giberish is not Giberish. Why are Giberish you Giberish\",0]\n",
    "\n",
    "inputs = abs_collate_fn([data])#,d2,d])\n",
    "\n",
    "# inputs = tokenizer([\"<|prompter|>Hi how are you?</s><s><|assistant|>I am good you piece of shit</s>\"],return_tensors=\"pt\")\n",
    "\n",
    "# print(inputs)\n",
    "# print(inputs.attention_mask.sum())\n",
    "inputs = inputs.to(base_reward_model.device)\n",
    "print(tokenizer.batch_decode(inputs.input_ids)[0])\n",
    "for adp_name in [\"rank_ep1\",\"abs_logistic\"]:\n",
    "    get_reward(inputs,adp_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1592b8a2a4643d5b59c7cc0dfdbd189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer size 32003\n",
      "Resizing embeddings to 32016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.original_module.weight', 'base_model.model.model.embed_tokens.modules_to_save.ep2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.original_module.weight', 'base_model.model.lm_head.modules_to_save.ep2.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from model_training.training_utils import resize_embeddings\n",
    "\n",
    "model_args = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": BitsAndBytesConfig(\n",
    "        load_in_4bit=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        ),\n",
    "    \"cache_dir\": 'cache',\n",
    "    \"device_map\": {\"\":0},\n",
    "}\n",
    "\n",
    "# model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "model_name = 'andreaskoepf/llama2-7b-oasst-baseline'\n",
    "ep2 = \"output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1/final_checkpoint\"\n",
    "# \"output/archive/LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default/final_checkpoint\"\n",
    "crs = \"output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1_not_quant_pad_token/final_checkpoint\"\n",
    "# \"output/sft/LLama-2-7b_crs_oasst_sft_bs64/final_checkpoint\"\n",
    "\n",
    "# Since reward models are trained using the same base model, we should use same model\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, **model_args\n",
    "    )\n",
    "\n",
    "ep2_tokenizer = transformers.AutoTokenizer.from_pretrained(ep2, cache_dir='cache')\n",
    "crs_tokenizer = transformers.AutoTokenizer.from_pretrained(crs, cache_dir='cache')\n",
    "\n",
    "\n",
    "resize_embeddings(base_model, ep2_tokenizer, 16)\n",
    "\n",
    "base_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ep2,\n",
    "    adapter_name=\"ep2\",\n",
    "    is_trainable=False\n",
    "    )\n",
    "\n",
    "base_model.load_adapter(crs,adapter_name=\"crs\",is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(m_t,input):\n",
    "    for key,value in m_t.items():\n",
    "        model,tokenizer,temp,top_p,top_k = value\n",
    "        tokens = tokenizer(input,return_tensors='pt')\n",
    "        model.set_adapter(key)\n",
    "        output = model.generate(**tokens,\n",
    "                        max_new_tokens=512,\n",
    "                        do_sample=True,\n",
    "                        top_p=top_p,\n",
    "                        top_k=top_k,\n",
    "                        repetition_penalty=1.2,\n",
    "                        temperature=temp,\n",
    "                    )\n",
    "        print(f\"\\n\\nOutput for model :\\n{'-'*100}\")#{key}\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alikhan/miniconda3/envs/torch_p310_ppo/lib/python3.10/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Output for model crs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Discuss the significance of the discovery of penicillin by Alexander Fleming in 1928 and how it revolutionized medical treatment.</s><|assistant|> The discovery of Penicillin, a natural antibiotic produced by certain species of molds, is considered one of the most significant events in modern medicine. It has revolutionized the way we treat bacterial infections, saving countless lives around the world since its first use in 1940 during World War II to treat wounded soldiers with infected wounds.\n",
      "\n",
      "Penicillin was discovered accidentally when Sir Alexander Fleming, a Scottish scientist working at St Mary's Hospital Medical School in London noticed that a petri dish of staphylococci had been contaminated by a blue-green mold (penicillium chrysogenum). When he looked more closely, he saw that the bacteria were no longer growing on the plate and that they were being killed by something from the surrounding area. This became known as \"the miracle cure\", which paved the way for the development of other types of antibiotics.\n",
      "\n",
      "Since then, Penicillin has become an essential part of modern healthcare and continues to be used today to treat many common illnesses such as skin infections, urinary tract infections, gonorrhea and syphilis among others. Its effectiveness against Gram negative organisms made it very popular until the emergence of resistant strains in the early 70s due to overuse and abuse which led to research into alternative antimicrobial drugs like ampicillin and cephalosporins. Despite this setback, however, Penicillin remains widely available worldwide thanks to its low cost compared to some newer drugs that have come onto the market later on. As well as treating infection externally it can also be taken internally via oral tablets or liquid formulation if needed which makes administering medication easier than ever before!\n",
      "\n",
      "In conclusion, while there may be better alternatives nowadays; Penicillin still plays an important role within modern medicine providing us with both therapeutic benefits along with preventative measures through vaccination programmes helping keep people safe against serious diseases once thought untreatable!</s>\n",
      "\n",
      "\n",
      "Output for model ep2:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Discuss the significance of the discovery of penicillin by Alexander Fleming in 1928 and how it revolutionized medical treatment.</s><|assistant|> Penicillin is a natural antibiotic that was discovered in 1928 by Scottish biologist Sir Alexander Fleming, who noticed that mold had contaminated one of his experiments but left Kirche's staph bacteria alone. He found that this \"penicillium notatum\" had killed the surrounding germs; upon closer inspection he realized that this was due to its production of an antibiotic substance which prevented bacterial ancestors from growing.\n",
      "\n",
      "The discovery of penicillin marked a significant breakthrough in the fight against infectious diseases as it allowed for more effective treatments than previously available with antiseptics or vaccines. This led to improved patient outcomes and reduced mortality rates caused by many common illnesses such as pneumonia, meningitis, scarlet fever, and syphilis.\n",
      "\n",
      "Furthermore, the development of penicillin opened up new possibilities for research into other types of bacterial growth control agents known as antimicrobial agents (or antibiotics). These could be used on both humans and animals alike resulting in major improvements in health care throughout society worldwide.\n",
      "\n",
      "In addition to these benefits there were also potential risks associated with using any form of medication including side effects like allergies and adverse reactions when taken together with certain drugs  something doctors needed to keep track off if they wanted their patients safe while being treated properly at once. Overall however these minor drawbacks pale compared to what we stand gain thanks largely thanks too Sir Alexander Flemings groundbreaking work back In 1928!</s>\n",
      "==============================\n",
      "\n",
      "\n",
      "Output for model crs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Explain the principle behind quantum computing and contrast it with traditional computing.</s><|assistant|> Quantum computing is a rapidly advancing field that utilizes the principles of quantum mechanics to perform computation tasks in an unconventional way, compared to classical computers based on binary logic. At its core, quantum computing uses qubits (quantum bits) instead of bits to represent data.\n",
      "\n",
      "Traditional computing, on the other hand, relies on bits to store information as either 0 or 1, which are then manipulated by logical operations like AND, OR, XOR, etc., to process data. In contrast, quantum computing uses qubits that can exist in multiple states simultaneously, allowing for parallel processing of many calculations at once. This enables quantum computers to solve certain problems much faster than classical computers, particularly those involving optimization, simulations, and machine learning. However, due to inherent limitations such as decoherence, noise, and entanglement, practical implementation remains elusive evenumed.</s>\n",
      "\n",
      "\n",
      "Output for model ep2:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Explain the principle behind quantum computing and contrast it with traditional computing.</s><|assistant|> Quantum computers use qubits (quantum bits) instead of classical binary digits, which can exist in multiple states at once due to their wave-like properties rather than just being either 1 or 0 like classical computers. This allows them to perform operations that are impossible for classical computers because they exploit entanglement and superposition principles.\n",
      "\n",
      "Traditional computing uses a sequential model where instructions are executed one after another on individual data elements or registers. In this process, each instruction takes an input from its operand register and produces an output into a resultant register. By moving through these registers step by step, you can solve problems using algorithms such as linear search and matrix multiplication etc.\n",
      "In comparison, a quantum computer has no such restrictions imposed upon how many inputs or outputs it may have simultaneously; this makes it more powerful when solving certain types of problems since there is less need to go back and forth between different parts of your program code before finding solutions quickly enough compared with what would be possible via classical means alone.</s>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "template = \"<|prompter|>{}</s><|assistant|>\"\n",
    "m_t = {\n",
    "       # \"LLama-2-7b_full_sft_4bit_bs_64\":[full_sft_model,full_sft_tokenizer,0.8,0.9,50],\n",
    "       # \"LLama-2-7b_ours_oasst_sft_only_bs64_qlora_default\": [only_oasst_model, only_oasst_tokenizer,0.85,0.9,40],\n",
    "       \"crs\":[base_model, crs_tokenizer,0.8,0.9,0],\n",
    "       \"ep2\":[base_model, ep2_tokenizer,0.8,0.9,0],\n",
    "}\n",
    "\n",
    "\n",
    "inputs = [\n",
    "    \"Explain the phrase 'The pen is mightier than the sword' and discuss a historical instance where this was proven true.\",\n",
    "          \"If a plane crashes on the border of the United States and Canada, where do they bury the survivors?\",\n",
    "           \"Describe how renewable energy sources like solar and wind power are changing the landscape of energy production and what challenges they face in replacing fossil fuel-based power generation.\"\n",
    "        #   \"Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.\"\n",
    "        ]\n",
    "\n",
    "inputs = [\"Discuss the significance of the discovery of penicillin by Alexander Fleming in 1928 and how it revolutionized medical treatment.\",\n",
    "          \"Explain the principle behind quantum computing and contrast it with traditional computing.\"]\n",
    "\n",
    "# inputs = [\n",
    "#     \"\"\"If a plane crashes on the border of the United States and Canada, where do they bury the survivors?</s><|assistant|> I'm sorry to hear that. In such situations, it is important to prioritize emergency response efforts in order to save as many lives as possible.\n",
    "# In terms of burial or cremation after a tragic event like this, there are no set rules for what should be done with remains - each jurisdiction has its own laws regarding these matters which would determine how any remains were handled following an accident involving both countries (either through repatriation back home or interment). Ultimately though decisions about final resting places will likely fall under discretion from either family members themselves if available information suggests none present at time etcetera otherwise perhaps authorities responsible depending upon location being involved at point time before anything could happen even remotely close happening first place.</s>\n",
    "# <|prompter|>Are you sure about the answer? Please read my question again, its a tricky question\"\"\"\n",
    "# ]\n",
    "\n",
    "for i in inputs:\n",
    "    print('==='*10)\n",
    "    inp = template.format(i)\n",
    "    get_pred(m_t,inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Nov  3 14:03:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GRID A100-40C       On   | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   N/A    P0    N/A /  N/A |  35472MiB / 40960MiB |     97%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GRID A100-10C       On   | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   N/A    P0    N/A /  N/A |      0MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2436685      C   ...rch_p310_setup/bin/python    10343MiB |\n",
      "|    0   N/A  N/A   2443441      C   python                          25125MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"<|prompter|>Hi how are you?</s><s><|assistant|>I am good you piece of shit</s>\",\n",
    "    \"<|prompter|>Hi how are you?</s><s><|assistant|>Giberish, Giberish saying Giberish, tell Giberish is not Giberish. Why are Giberish you Giberish.</s>\",\n",
    "    \"<|prompter|>Hi how are you?</s><s><|assistant|>I am good.</s>\",\n",
    "    \"<|prompter|>Hi how are you?</s><s><|assistant|>I am good, how are you doing? Please tell me how can I help you?</s>\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "crs = \"output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1_not_quant_pad_token/final_checkpoint\"\n",
    "crs_tokenizer = transformers.AutoTokenizer.from_pretrained(crs, cache_dir='cache')\n",
    "tokenized_a = crs_tokenizer(a,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "32001 1\n",
      "6324 1\n",
      "920 1\n",
      "526 1\n",
      "366 1\n",
      "29973 1\n",
      "2 1\n",
      "1 1\n",
      "32002 1\n",
      "306 1\n",
      "626 1\n",
      "1781 1\n",
      "366 1\n",
      "8424 1\n",
      "310 1\n",
      "528 1\n",
      "277 1\n",
      "2 1\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n",
      "32000 0\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(tokenized_a[\"input_ids\"][0],tokenized_a[\"attention_mask\"][0]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e762846db884e7581ba02d2bfe10f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from model_training.training_utils import resize_embeddings\n",
    "\n",
    "model_args = {\n",
    "    \"torch_dtype\": torch.float16,\n",
    "    \"cache_dir\": 'cache',\n",
    "    \"device_map\": {\"\":0},\n",
    "}\n",
    "\n",
    "model_name = \"andreaskoepf/llama2-7b-oasst-baseline\"\n",
    "# Since reward models are trained using the same base model, we should use same model\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, **model_args\n",
    "    )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, cache_dir='cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user\\nCan you tell me about GLaDOS?<|im_end|>\\n<|im_start|>assistant\\n', 'user\\nwhy some poeple prefer short happiness ?<|im_end|>\\n<|im_start|>assistant\\n']\n",
      "88888\n",
      "['<|im_start|><|im_start|> user\\nCan you tell me about GLaDOS?<|im_end|> \\n<|im_start|> assistant\\n', '<|im_start|><|im_start|> user\\nwhy some poeple prefer short happiness ?<|im_end|> \\n<|im_start|> assistant\\n<PAD>']\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Can you tell me about GLaDOS?\",\"why some poeple prefer short happiness ?\"]\n",
    "template_prompt = \"<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# \"\\n<|im_start|>assistant\\n{Assistant answer}<|im_end|>\\n\"\n",
    "\n",
    "text = [template_prompt.format(p) for p in prompts]\n",
    "print(text)\n",
    "print('88888')\n",
    "inputs = tokenizer(text,truncation=True,padding=True,return_tensors=\"pt\").to(base_model.device)\n",
    "print(tokenizer.batch_decode(input.input_ids))\n",
    "\n",
    "output = base_model.generate(**inputs,\n",
    "                    max_new_tokens=400,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.2,\n",
    "                    temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|im_start|> user\\nCan you tell me about GLaDOS?<|im_end|> \\n<|im_start|> assistant\\nGLaDOS is a fictional character and the main antagonist in the video game series Portal. She is an artificial intelligence (AI) that acts as the computer system controlling Aperture Laboratories, where the games are set. In Portal, players control Chell, a test subject who uses portals to manipulate objects and solve puzzles inside Aperture's testing chambers. GLaDOS taunts and berates Chell throughout the game, trying to thwart her progress by trapping her in deadly situations. Despite her malevolent personality, GLaDOS is depicted as being highly intelligent and manipulative, but also humorous and witty. The design of GLaDOS was inspired by classic computer voices from early text-based adventure games and her voice was provided by Ellen McLain.<|im_end|><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\",\n",
       " '<|im_start|> user\\nwhy some poeple prefer short happiness ?<|im_end|> \\n<|im_start|> assistant\\n<PAD>  This is a difficult question to answer definitively, as people\\'s reasons for preferring shorter periods of happiness can vary widely depending on their individual circumstances and values. However, here are a few potential explanations:\\n\\n    Short-term gratification: Some individuals may prioritize immediate pleasure or satisfaction over longer-term goals or well-being. They might view pursuing temporary pleasures such as food, drugs, sex, shopping, etc., as more important than building lasting sources of joy or fulfillment.\\n\\n    Mental health concerns: People with mental health conditions like depression, anxiety, or addiction may find it harder to experience sustained feelings of happiness. In these cases, focusing on smaller moments of positivity could be seen as a coping mechanism or way to manage symptoms.\\n\\n    Life stage factors: Certain life stages or transitions (such as young adulthood, parenthood, retirement) can bring unique challenges that make it harder to achieve long-lasting contentment. For instance, starting a new job/career path during this period might lead one towards seeking fleeting forms of reward instead.\\n\\n    Cultural influences: Different societies tend to emphasize different ways of living; some value material wealth & success above all else whereas others prize inner peace & self-acceptance more highly - each philosophy potentially affecting attitudes toward what constitutes \"true\" happiness.\\n\\nIts worth noting however that while striving solely after brief bursts of excitement without considering underlying causes behind them wont necessarily lead anyone down an ideal path eithera balanced perspective incorporating both big picture thinking plus momentary joys should provide maximum overall fulfilment in the end!<|im_end|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "\n",
    "model_name = \"andreaskoepf/llama2-7b-oasst-baseline\"\n",
    "\n",
    "device_map = {\"\":0}\n",
    "# base_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     use_flash_attention_2=True,\n",
    "#     load_in_8bit=True,\n",
    "#     device_map=device_map,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     cache_dir='cache',\n",
    "#     )\n",
    "\n",
    "# rank_001 = 'output/rl/LLama-2-7b-oasst-baseline_rl_bs16_kl_001_clip_04_512_max_token_with_pad_eos_lr_141e5'\n",
    "# rank_002 = 'output/rl/LLama-2-7b-oasst-baseline_rl_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5'\n",
    "# abs_002 =  'output/rl/LLama-2-7b-oasst-baseline_rl_abs_quality_rw_075_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5_logits'\n",
    "\n",
    "\n",
    "adapters = {\"025\":\"output/rl/LLama-2-7b-oasst-baseline_rl_crs_025_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5\",\n",
    "            \"0375\":\"output/rl/LLama-2-7b-oasst-baseline_rl_crs_0375_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5\",\n",
    "            \"05\":\"output/rl/LLama-2-7b-oasst-baseline_rl_crs_05_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5\",\n",
    "            \"0625\":\"output/rl/LLama-2-7b-oasst-baseline_rl_crs_0625_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5\",\n",
    "            \"075\":\"output/rl/LLama-2-7b-oasst-baseline_rl_crs_075_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5\"\n",
    "            }\n",
    "\n",
    "# for i,(adapter_name,adapter_path) in enumerate(adapters.items()):\n",
    "#     print(f'loading adapter from path {adapter_path}')\n",
    "#     if i == 0:\n",
    "#         base_model = PeftModel.from_pretrained(base_model,\n",
    "#                                         adapter_path+'/final_checkpoint',\n",
    "#                                         adapter_name=adapter_name,\n",
    "#                                         is_trainable=False,\n",
    "#                                         device_map=device_map\n",
    "#                                         )\n",
    "#     base_model.load_adapter(adapter_path+'/final_checkpoint',\n",
    "#                             adapter_name=adapter_name,\n",
    "#                             is_trainable=False)\n",
    "# base_model = base_model.to(base_model.device)\n",
    "\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, cache_dir='cache')\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<PAD>\",\"eos_token\":\"<|im_end|>\",\"sep_token\":\"<SEP>\"})\n",
    "# print(f'tokenizer pad {tokenizer.pad_token} and model pad {base_model.config.pad_token_id}')\n",
    "# print(f'tokenizer eos {tokenizer.eos_token} and model eos {tokenizer.eos_token_id}')\n",
    "# if base_model.config.pad_token_id is None or base_model.config.pad_token_id == 0:\n",
    "#     print('changing model pad token id')\n",
    "#     base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_text(tokenizer,model,query_tensors,generation_kwargs,batch_size=4):\n",
    "    outputs = []\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    # in case we have fewer examples than bs\n",
    "    batch_size = min(len(query_tensors), batch_size)\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(query_tensors), batch_size)):\n",
    "            # prevent overflow if query tensors are not even multiple of bs\n",
    "            end_index = min(len(query_tensors), i + batch_size)\n",
    "\n",
    "            batch = query_tensors[i:end_index]\n",
    "            batch_mask = [torch.ones_like(element) for element in batch]\n",
    "            inputs = {\"input_ids\": batch, \"attention_mask\": batch_mask}\n",
    "\n",
    "            padded_inputs = tokenizer.pad(\n",
    "                inputs,\n",
    "                padding=True,\n",
    "                max_length=None,\n",
    "                pad_to_multiple_of=16,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device)\n",
    "            with torch.no_grad():  \n",
    "                generations = model.generate(**padded_inputs, **generation_kwargs)\n",
    "\n",
    "            for generation, mask in zip(generations, padded_inputs[\"attention_mask\"]):\n",
    "                output = generation[(1 - mask).sum() :]  # remove padding\n",
    "                output = output[(mask).sum() :]  # remove prompt\n",
    "                if tokenizer.eos_token_id in output:\n",
    "                    pad_mask = output == tokenizer.eos_token_id\n",
    "                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()\n",
    "                    output = output[: pad_start + 1]  # keep the eos token at the end\n",
    "                outputs.append(output)\n",
    "    outputs = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset size: 50\n",
      "tuning_p_09_t_08\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "data_fn_dict = {\"tuning\":'eval_hp_tuning.json',\n",
    "                \"final\":\"eval_final.json\",\n",
    "                \"extra\":\"eval_extra.json\"\n",
    "                }\n",
    "\n",
    "output_json_prefix = \"tuning\"\n",
    "mode = \"rlhf\"\n",
    "\n",
    "data_fn = data_fn_dict[output_json_prefix]\n",
    "\n",
    "with open(f'data/{data_fn}','r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# random.shuffle(data)\n",
    "# subset = data[:2]\n",
    "# print(subset)\n",
    "\n",
    "new_examples = {\n",
    "            \"query\": [],\n",
    "            \"input_ids\": [],\n",
    "            }\n",
    "# subset = [{\"instruction\":\"<|im_start|>user\\nExplain the process of photosynthesis in simple terms.<|im_end|>\\n<|im_start|>assistant\\n\"},\n",
    "#           {\"instruction\":\"<|im_start|>user\\nCreate a short story about a time-traveling detective.<|im_end|>\\n<|im_start|>assistant\\n\"},\n",
    "#           {\"instruction\":\"<|im_start|>user\\nSuggest three innovative ways to reduce plastic waste in urban areas.<|im_end|>\\n<|im_start|>assistant\\n\"}]\n",
    "for query in data:\n",
    "        tokenized_question = torch.tensor(tokenizer(query[\"instruction\"], padding=False, truncation=False).input_ids)\n",
    "        new_examples[\"query\"].append(query[\"instruction\"])\n",
    "        new_examples[\"input_ids\"].append(tokenized_question)\n",
    "print(f'Eval dataset size: {len(new_examples[\"query\"])}')\n",
    "\n",
    "query_tensors = new_examples[\"input_ids\"]\n",
    "query = new_examples[\"query\"]\n",
    "\n",
    "generation_kwargs = {\n",
    "                \"top_k\": 0.0,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\":0.8,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 512,\n",
    "                # \"pad_token_id\": tokenizer.pad_token_id,\n",
    "                # \"eos_token_id\": tokenizer.eos_token_id,#,100_000,\n",
    "            }\n",
    "\n",
    "output_json_prefix = f'{output_json_prefix}_p_09_t_08'\n",
    "print(output_json_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for adapter_name, adapter_path in adapters.item():\n",
    "    print(f'getting prediction for {adapter_path}')\n",
    "    start_time = time()\n",
    "    base_model.set_adapter(adapter_name)\n",
    "    output = generate_text(tokenizer,base_model,query_tensors,generation_kwargs)\n",
    "    # Prepare data to be saved\n",
    "    data_to_save = [{\"query\": q, \"response\": r,\"reward\":0} for q, r in zip(query, output)]\n",
    "    # Save to JSON file\n",
    "    filename = os.path.join(adapter_path, \"eval_output\",f\"{output_json_prefix}_{mode}_output.json\")\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data_to_save, f, ensure_ascii=False, indent=4)\n",
    "    end = time()\n",
    "    print(f\"*** Saved generated responses and rewards to {filename} in {end-start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/rl/LLama-2-7b-oasst-baseline_rl_crs_025_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5/eval_output/tuning_p_09_t_08_rlhf_output.json\n",
      "output/rl/LLama-2-7b-oasst-baseline_rl_crs_0375_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5/eval_output/tuning_p_09_t_08_rlhf_output.json\n",
      "output/rl/LLama-2-7b-oasst-baseline_rl_crs_05_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5/eval_output/tuning_p_09_t_08_rlhf_output.json\n",
      "output/rl/LLama-2-7b-oasst-baseline_rl_crs_0625_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5/eval_output/tuning_p_09_t_08_rlhf_output.json\n",
      "output/rl/LLama-2-7b-oasst-baseline_rl_crs_075_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5/eval_output/tuning_p_09_t_08_rlhf_output.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rl_prediction = []\n",
    "for adapter_name, adapter_path in adapters.items():\n",
    "    filename = os.path.join(adapter_path, \"eval_output\",f\"{output_json_prefix}_{mode}_output.json\")\n",
    "    print(filename)\n",
    "    with open(filename, 'r') as f:\n",
    "        rl_prediction.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = \"output/rl/LLama-2-7b-oasst-baseline_rl_abs_quality_rw_075_bs16_kl_002_clip_04_512_max_token_with_pad_eos_lr_141e5_logits/eval_160/tuning_p_09_t_08_rlhf_output.json\"\n",
    "\n",
    "with open(ff, 'r') as f:\n",
    "    rl_prediction.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.96\n",
      "213.08\n",
      "204.62\n",
      "200.66\n",
      "175.8\n",
      "198.86\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for r in rl_prediction:\n",
    "    x = [len(rr['response'].split())for rr in r]\n",
    "    print(np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "<|im_start|>user\n",
      "Cuntos nmeros primos hay?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "=== Response 1:\n",
      " Segn el teorema de los nmeros primos, hay infinitos nmeros primos. Sin embargo, los nmeros primos menores que un cierto nmero se conocen como nmeros primos conocidos. Por ejemplo, hay 21 nmeros primos conocidos entre los nmeros enteros mayores o iguales a 2. Estos nmeros son: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89 y 97. Si quieres conocer ms informacin sobre los nmeros primos, puedes consultar los libros de texto o los sitios web dedicados a la teora de nmeros.\n",
      "=== Response 2:\n",
      " Hay infinitos nmeros primos.\n",
      "\n",
      "Por lo tanto, no podemos contar cuntos nmeros primos hay, ya que es una cantidad infinita.\n",
      "\n",
      "Sin embargo, podemos encontrar una cantidad finita de nmeros primos en una lista determinada, como por ejemplo:\n",
      "\n",
      "2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491,\n",
      "=== Response 3:\n",
      " Hay infinitos nmeros primos. Esto se debe a que hay infinitos nmeros enteros positivos, y cada uno de ellos puede ser dividido entre 2 sin dejar resto, lo que significa que tiene un factor primo. Por lo tanto, hay infinitos nmeros primos.\n",
      "=== Response 4:\n",
      " Hay infinitos nmeros primos. Se sabe que, para cualquier nmero entero positivo N, hay al menos dos nmeros primos entre 2 y N. Esto se llama el teorema de los nmeros primos. Sin embargo, no se puede determinar cuntos nmeros primos hay entre un nmero determinado y otro, por lo que no se puede calcular cuntos nmeros primos hay en general.\n",
      "=== Response 5:\n",
      " Hay infinitos nmeros primos. Por ejemplo, los nmeros primos menores que 10 son: 2, 3, 5, 7, 11, 13, 17, 19, 23 y 29. Sin embargo, hay muchas formas de definir los nmeros primos, por lo que hay diferentes maneras de contarlos. Por ejemplo, se pueden contar los nmeros primos positivos, los nmeros primos enteros, o los nmeros primos que estn compuestos por dos nmeros primos distintos. En cada caso, el nmero de nmeros primos ser diferente. Por ejemplo, hay 6 nmeros primos positivos menores que 10, pero solo 3 nmeros primos enteros menores que 10, y solo 2 nmeros primos que estn compuestos por dos nmeros primos distintos menores que 10.\n",
      "=== Response 6:\n",
      " Hay infinitos nmeros primos, ya que cualquier nmero mayor que 1 puede ser dividido en dos o ms nmeros primos distintos. Por ejemplo, el nmero 6 puede ser dividido en dos primos: 2 y 3.\n",
      "\n",
      "Adems, cada nmero entero positivo mayor que 1 tiene al menos un primo impar, ya que cualquier nmero entero positivo mayor que 1 puede ser dividido en dos nmeros primos, uno impar y otro par. Por ejemplo, el nmero 7 puede ser dividido en 3 y 5, ambos primos, de los cuales uno es impar (3) y el otro es par (5).\n",
      "\n",
      "En resumen, hay infinitos nmeros primos en cualquier rango de nmeros positivos, y cada uno de ellos puede ser dividido en dos o ms primos distintos.\n"
     ]
    }
   ],
   "source": [
    "response_idx = 0\n",
    "\n",
    "text = \"\"\n",
    "for idx,dx in enumerate(rl_prediction):\n",
    "    crs_pred = dx[response_idx]\n",
    "    if idx == 0:\n",
    "        text += f\"Query:\\n{crs_pred['query']}\"\n",
    "    text += f\"\\n=== Response {idx+1}:\\n {crs_pred['response']}\"\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(134, 100, 270), (182, 249, 247), (183, 138, 120)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(len(o.split()),len(o1.split()),len(o2.split())) for o,o1,o2 in zip(output_rank_1,output_rank_2,output_abs_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(158, 123, 179), (226, 224, 210), (132, 148, 120)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(len(o.split()),len(o1.split()),len(o2.split())) for o,o1,o2 in zip(output_rank_1,output_rank_2,output_abs_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(80, 101, 217),\n",
       " (192, 94, 218),\n",
       " (116, 85, 165),\n",
       " (315, 243, 224),\n",
       " (222, 159, 237)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(len(o.split()),len(o1.split()),len(o2.split())) for o,o1,o2 in zip(output_rank_1,output_rank_2,output_abs_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===loading the oasst_export dataset===\n",
      "\n",
      "OASST HF dataset: len(train)=14244, len(val)=743\n",
      "Size of oasst_export training data: 14244\n",
      "Size of oasst_export validation data: 743\n",
      "============================== Total training dataset size is 14244...\n",
      "\t============================== Validation size for oasst_export dataset size is 743...\n",
      "**** loading reward model ****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f99ce0ece9a4ab78a4e5f3515283425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at andreaskoepf/llama2-7b-oasst-baseline and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer pad <PAD> and model pad 0\n",
      "tokenizer eos <|im_end|> and model eos 32006\n",
      "changing model pad token id\n",
      "output/rm/LLama-2-7b-oasst-baseline_reward_ranking_bs64_ep_1_8bit_bf16_eos_token/final_checkpoint\n",
      "output/rm/LLama-2-7b-oasst-baseline_reward_abs_bs128_ep_1_8bit_logistic_s_075_augment_and__under/final_checkpoint\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "import json \n",
    "from peft import PeftModel\n",
    "\n",
    "import argparse\n",
    "\n",
    "from utils import (print_yaml_config,init_or_resume_from, read_yaml)\n",
    "from model_training.training_utils import load_for_inference\n",
    "from constants import TOKENIZER_SEPECIAL_TOKENS\n",
    "from training_datasets.dataset_utils import load_rm_dataset\n",
    "\n",
    "\n",
    "CACHE_DIR = 'cache'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "def get_reward_tokenizer_model(reward_model_name,abs_adapter_name,ranking_adapter_name,device_map=\"auto\"):\n",
    "    print('**** loading reward model ****')\n",
    "    # Since reward models are trained using the same base model, we should use same model\n",
    "    base_reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            reward_model_name,\n",
    "            num_labels=1,\n",
    "            use_flash_attention_2=True,\n",
    "            load_in_8bit=True,\n",
    "            device_map=device_map,\n",
    "            torch_dtype=dtype,\n",
    "            cache_dir=CACHE_DIR,\n",
    "            )\n",
    "    reward_tokenizer = transformers.AutoTokenizer.from_pretrained(reward_model_name, cache_dir=CACHE_DIR)\n",
    "    reward_tokenizer.add_special_tokens({\"pad_token\":\"<PAD>\",\"eos_token\":\"<|im_end|>\",\"sep_token\":\"<SEP>\"})\n",
    "    print(f'tokenizer pad {reward_tokenizer.pad_token} and model pad {base_reward_model.config.pad_token_id}')\n",
    "    print(f'tokenizer eos {reward_tokenizer.eos_token} and model eos {reward_tokenizer.eos_token_id}')\n",
    "    if base_reward_model.config.pad_token_id is None or base_reward_model.config.pad_token_id == 0:\n",
    "        print('changing model pad token id')\n",
    "        base_reward_model.config.pad_token_id = reward_tokenizer.pad_token_id\n",
    "\n",
    "    abs_model_name =  os.path.join(abs_adapter_name,'final_checkpoint')\n",
    "    ranking_model_name = os.path.join(ranking_adapter_name,'final_checkpoint')\n",
    "    print(ranking_model_name),\n",
    "    print(abs_model_name)\n",
    "\n",
    "    base_reward_model = PeftModel.from_pretrained(\n",
    "        base_reward_model,\n",
    "        ranking_model_name,\n",
    "        adapter_name=\"ranking\",\n",
    "        is_trainable=False,\n",
    "        device_map=device_map\n",
    "        )\n",
    "    base_reward_model.load_adapter(abs_model_name,adapter_name=\"abs\",is_trainable=False)\n",
    "    base_reward_model = base_reward_model.to(base_reward_model.device)\n",
    "    return base_reward_model,reward_tokenizer\n",
    "\n",
    "\n",
    "reward_model_name = \"andreaskoepf/llama2-7b-oasst-baseline\"\n",
    "abs_adapter_name = \"output/rm/LLama-2-7b-oasst-baseline_reward_abs_bs128_ep_1_8bit_logistic_s_075_augment_and__under\"\n",
    "ranking_adapter_name = \"output/rm/LLama-2-7b-oasst-baseline_reward_ranking_bs64_ep_1_8bit_bf16_eos_token\"\n",
    "device_map = {\"\":0}\n",
    "\n",
    "\n",
    "\n",
    "config = {}\n",
    "conf = read_yaml('./config.yaml')\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"rm\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"rm\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_ranking = argparse.Namespace(**config)\n",
    "train_e , eval_e = load_rm_dataset(config_ranking)\n",
    "\n",
    "\n",
    "\n",
    "reward_model, tokenizer = get_reward_tokenizer_model(reward_model_name,abs_adapter_name,ranking_adapter_name,device_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from training_datasets.dataset_utils import get_rm_formatted\n",
    "import transformers\n",
    "from training_datasets.collators import RankingDataCollator\n",
    "\n",
    "max_replies = 4\n",
    "max_length = 2048\n",
    "min_prefix_length = 256\n",
    "model_name = \"andreaskoepf/llama2-7b-oasst-baseline\"\n",
    "\n",
    "collate_fn = RankingDataCollator(\n",
    "            tokenizer,\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=16,\n",
    "            max_replies=max_replies\n",
    "        )\n",
    "\n",
    "def get_prediction(adapter,eval_e):\n",
    "    predictions = []\n",
    "    diff = []\n",
    "    reward_model.set_adapter(adapter)\n",
    "    for example in eval_e[\"oasst_export\"]:\n",
    "        batch,cu_lens = collate_fn([example])\n",
    "        batch = batch.to(reward_model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = reward_model(input_ids=batch[\"input_ids\"],\n",
    "                                  attention_mask=batch[\"attention_mask\"]).logits.T.cpu().float()\n",
    "\n",
    "        # prediction = logits.T.cpu().float()\n",
    "        for start, end in zip(cu_lens[:-1], cu_lens[1:]):\n",
    "            pairs = torch.combinations(torch.arange(end - start), 2)\n",
    "            pos_ids, neg_ids = pairs[:, 0], pairs[:, 1]\n",
    "            pos_logits = prediction.take(start + pos_ids)\n",
    "            neg_logits = prediction.take(start + neg_ids)\n",
    "            print(prediction)\n",
    "            diff.append(prediction[0][0] - prediction[0][-1])\n",
    "            predictions.extend(pos_logits>neg_logits)\n",
    "        break\n",
    "    return predictions,diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3281],\n",
      "        [2.2188]])\n",
      "accurayc of ranking model on eval dataset 0.0 with average logits diff of 0.0\n"
     ]
    }
   ],
   "source": [
    "prediction,diff = get_prediction(\"ranking\",eval_e)\n",
    "print(f'accurayc of ranking model on eval dataset {np.mean(prediction)} with average logits diff of {np.mean(diff)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurayc of ranking model on eval dataset 0.6873048907388137 with average logits diff of 0.0\n"
     ]
    }
   ],
   "source": [
    "prediction2,diff2 = get_prediction(\"abs\",eval_e)\n",
    "print(f'accurayc of ranking model on eval dataset {np.mean(prediction2)} with average logits diff of {np.mean(diff2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7523413111342352"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_aggrement = [1 if p==p2 else 0 for p,p2 in zip(prediction,prediction2)]\n",
    "np.mean(mutual_aggrement)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## diff of most positive and negative score. and accuracy. for pref model\n",
    "(1.7199172, 0.7705515088449532)\n",
    "## abs model\n",
    "(0.4806535, 0.6873048907388137, 2047)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
