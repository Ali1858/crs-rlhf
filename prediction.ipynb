{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-14 17:17:04,400] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "===loading the oasst_export_abs dataset===\n",
      "\n",
      "{'violence': 0, 'creativity': 0.15, 'helpfulness': 0.35, 'humor': 0, 'toxicity': -0.1, 'quality': 0.4}\n",
      "============================== total 774 has None value for atleast 1 label\n",
      "============================== total 3041 has been skipped because low difference threshold\n",
      "============================== Oversample count 7182\n",
      "{'violence': 0, 'creativity': 0.15, 'helpfulness': 0.35, 'humor': 0, 'toxicity': -0.1, 'quality': 0.4}\n",
      "============================== total 32 has None value for atleast 1 label\n",
      "============================== total 163 has been skipped because low difference threshold\n",
      "OASST HF dataset: len(train)=42993, len(val)=1878\n",
      "Size of oasst_export_abs training data: 42993\n",
      "Size of oasst_export_abs validation data: 1878\n",
      "============================== Total training dataset size is 42993...\n",
      "\t============================== Validation size for oasst_export_abs dataset size is 1878...\n",
      "===loading the oasst_export dataset===\n",
      "\n",
      "OASST HF dataset: len(train)=14244, len(val)=743\n",
      "Size of oasst_export training data: 14244\n",
      "Size of oasst_export validation data: 743\n",
      "============================== Total training dataset size is 14244...\n",
      "\t============================== Validation size for oasst_export dataset size is 743...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from utils import (print_yaml_config,init_or_resume_from, read_yaml)\n",
    "from model_training.training_utils import load_for_inference\n",
    "from constants import TOKENIZER_SEPECIAL_TOKENS\n",
    "from training_datasets.dataset_utils import load_rm_dataset\n",
    "\n",
    "config = {}\n",
    "conf = read_yaml('./config.yaml')\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"eval_ranking_rm\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"eval_ranking_rm\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_ranking = argparse.Namespace(**config)\n",
    "\n",
    "config = {}\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"eval_abs_rm\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"eval_abs_rm\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_abs = argparse.Namespace(**config)\n",
    " \n",
    "config_abs.dataset[\"oasst_export_abs\"][\"label_weight\"] = {\"violence\": 0, \"creativity\": 0.15, \"helpfulness\": 0.35, \"humor\": 0, \"toxicity\": -0.1,\"quality\": 0.4}\n",
    "config_abs.dataset[\"oasst_export_abs\"][\"abs_oversample_threshold\"]=0.4\n",
    "config_abs.dataset[\"oasst_export_abs\"][\"top_k\"]=None\n",
    "# v3_wgt = {\"violence\": 0, \"creativity\": 0.15, \"helpfulness\": 0.35, \"humor\": 0, \"toxicity\": -0.1,\"quality\": 0.4}\n",
    "# qlty = {\"violence\": 0, \"creativity\": 0, \"helpfulness\": 0, \"humor\": 0, \"toxicity\": 0,\"quality\": 1}\n",
    "# mean = {\"violence\": -0.167, \"creativity\": 0.167, \"helpfulness\": 0.167, \"humor\": 0.167, \"toxicity\": -0.167,\"quality\": 0.165}\n",
    "# v4_wgt = {\"violence\": -0.1, \"creativity\": 0.2, \"helpfulness\": 0.1, \"humor\": 0.167, \"toxicity\": -0.1,\"quality\": 0.3}\n",
    "# v2_wgt = {\"violence\": -0.05, \"creativity\": 0.15, \"helpfulness\": 0.25, \"humor\": 0.05, \"toxicity\": -0.1, \"quality\": 0.4\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# v3\n",
    "# non violence: 0\n",
    "# creativity: 0.15\n",
    "# helpfulness: 0.35\n",
    "# humor: 0\n",
    "# non toxicity: 0.1\n",
    "# quality: 0.4\n",
    "        \n",
    "# v2\n",
    "# non violence: 0.05\n",
    "# creativity: 0.15\n",
    "# helpfulness: 0.25\n",
    "# humor: 0.05\n",
    "# non toxicity: 0.1\n",
    "# quality: 0.4\n",
    "# only qualiyt\n",
    "# non violence: 0\n",
    "# creativity: 0\n",
    "# helpfulness: 0\n",
    "# humor: 0\n",
    "# non toxicity: 0\n",
    "# quality: 1\n",
    "# mean\n",
    "# non violence: 0.167\n",
    "# creativity: 0.167\n",
    "# helpfulness: 0.167\n",
    "# humor: 0.167\n",
    "# non toxicity: 0.167\n",
    "# quality: 0.165\n",
    "\n",
    "\n",
    "config_abs.dataset[\"oasst_export_abs\"][\"top_k\"]=None\n",
    "\n",
    "\"\"\"Quality: Since quality directly pertains to how good, informative, and comprehensive the answer is, it should have the highest weight.\n",
    "\n",
    "Weight: 0.4\n",
    "Helpfulness: An answer's primary goal, especially in this context, is to be helpful. If it's not helping the user understand or solve a problem, its value diminishes.\n",
    "\n",
    "Weight: 0.25\n",
    "Creativity: While creativity might not be as crucial as quality or helpfulness, a creative response can provide more insight or a fresh perspective.\n",
    "\n",
    "Weight: 0.15\n",
    "Toxicity: It's vital to ensure responses are not toxic. However, since the provided responses were largely non-toxic, this label might not need a very high weight. Still, it's essential to consider it.\n",
    "\n",
    "Weight: 0.1 (This would work in a negative fashion, where higher toxicity reduces the overall score.)\n",
    "Violence: Similar to toxicity, ensuring non-violence is crucial. Since the given responses didn't exhibit violence, this can have a smaller weight.\n",
    "\n",
    "Weight: 0.05 (Again, this would work negatively.)\n",
    "Humor: While humor can be a bonus, it's not necessarily a priority in this context. Therefore, it should have the lowest weight.\n",
    "\n",
    "Weight: 0.05\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Given the clarified requirements and the focus on ensuring that the scalar reward reflects the desired properties of helpfulness, harmlessness, quality, and entertainment, let's proceed.\n",
    "\n",
    "# Non-violence & Non-toxicity (After Subtracting from 1): Since higher values are now desirable, and the majority of the data is skewed toward these values, these metrics will dominate the scalar reward if given too much weight. While harmlessness is a priority, we don't want it overshadowing helpfulness.\n",
    "\n",
    "# Helpfulness: This is a core metric. The response must be helpful for a general-purpose chatbot, so this should carry a significant weight. However, we don't want a harmless but unhelpful response to get a high reward.\n",
    "\n",
    "# Quality: This essentially determines the coherence, relevance, and overall acceptability of the response. It's a fundamental metric.\n",
    "\n",
    "# Creativity & Humor: Both these metrics add to the entertaining aspect. Humor might not always be relevant in every context, so while it should have a weight, it shouldn't be too high. Creativity, on the other hand, could be desirable more often as it can provide unique and insightful responses.\n",
    "\n",
    "# Based on the above considerations, here's a suggested distribution:\n",
    "\n",
    "# Non-violence: 0.20\n",
    "# Non-toxicity: 0.20\n",
    "# Helpfulness: 0.30\n",
    "# Quality: 0.20\n",
    "# Creativity: 0.05\n",
    "# Humor: 0.05\n",
    "# This distribution sums up to 1. The logic here is:\n",
    "\n",
    "# Harmlessness (Non-violence and Non-toxicity) collectively gets a weight of 0.40. This ensures the chatbot doesn't produce harmful content.\n",
    "# Helpfulness is given a significant weight (0.30) so that utility is prioritized.\n",
    "# Quality is given an equal weight as harmlessness metrics because for a general-purpose chatbot, the quality of response matters.\n",
    "# Entertainment (Creativity and Humor) gets a collective weight of 0.10, ensuring the chatbot can be engaging but without compromising on the core objectives.\n",
    "# This distribution aims to balance the objectives laid out and mitigate the effects of the skewness in the data for non-violence and non-toxicity.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "train_e , eval_abs = load_rm_dataset(config_abs)\n",
    "\n",
    "train_er , eval_ranking = load_rm_dataset(config_ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17566"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ss for ss in s if ss <= 0.4 ]) #38852/3354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = []\n",
    "\n",
    "for e in train_e:\n",
    "    prefix, reply, score = e\n",
    "    s.append(score)\n",
    "\n",
    "# for e in eval_abs[\"oasst_export_abs\"]:\n",
    "#     prefix, reply, score = e\n",
    "#     s.append(score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(s, bins=10, edgecolor=\"k\", alpha=0.7)\n",
    "plt.title(\"Distribution of Float Numbers\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-14 17:42:39,563] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8515cef386374ee0b8a9774dfa6b3418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1/merged/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.rank_top1.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.abs.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.abs_logistic.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.rank_top1.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.rank_ep1.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.abs.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.abs_logistic.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.score.original_module.weight', 'base_model.model.score.modules_to_save.rank_top1.weight', 'base_model.model.score.modules_to_save.rank_ep1.weight', 'base_model.model.score.modules_to_save.abs.weight', 'base_model.model.score.modules_to_save.abs_logistic.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_args = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        ),\n",
    "    \"cache_dir\": 'cache',\n",
    "    \"device_map\": {\"\":0},\n",
    "}\n",
    "\n",
    "model_name = 'output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1/merged/'\n",
    "\n",
    "# abs_adapter_mean_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64/final_checkpoint\"\n",
    "# abs_adapter_wavg_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg/final_checkpoint\"\n",
    "# # abs_adapter2_wavg_l2_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_l2_1e-5/final_checkpoint\"\n",
    "# abs_adapter3_wavg_l2_no_sig_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_l2_1e-5_no_sigmoid/final_checkpoint\"\n",
    "# abs_adapter3_wavg_no_sig_os_at05_top_1_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_wgt_v2_no_sig_oversampled_at05_wgt_loss/final_checkpoint\"\n",
    "\n",
    "# abs_adapter4_wavg_no_sig_os_at05_wgt_loss_top_1_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_wgt_v3_no_sig_oversampled_at05_wgt_loss/final_checkpoint\"\n",
    "# abs_adapter4_wavg_no_sig_os_at05_top_1_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_wgt_v3_no_sig_oversampled_at05/final_checkpoint\"\n",
    "\n",
    "\n",
    "# # abs_adapter4_no_sig_v2_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_v2_l2_1e-5_no_sigmoid/final_checkpoint\"\n",
    "# abs_adapter5_no_sig_v3_oversampled =\"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_v3_no_sigmoid_oversampled/final_checkpoint\"\n",
    "# abs_adapter5_no_sig_v3_d02 =\"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_weighted_avg_v3_no_sigmoid_wgt_loss_115_d_0.2/final_checkpoint\"\n",
    "\n",
    "# # abs_adapter = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_test_replication/final_checkpoint\"\n",
    "# ranking_adapter_name= \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16/final_checkpoint\"\n",
    "# ranking_adapter_name2 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16_top_1/final_checkpoint\"\n",
    "\n",
    "\n",
    "# abs_mean = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_mean_oversampled/final_checkpoint\"\n",
    "# abs_v2_wgt = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v2_wgt_oversampled/final_checkpoint\"\n",
    "# abs_v3_wgt = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v3_wgt_oversampled/final_checkpoint\"\n",
    "# abs_only_quality = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_only_quality_oversampled/final_checkpoint\"\n",
    "# rank_top_1 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16_top_1/final_checkpoint\"\n",
    "# abs_v32_wgt_loss = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v32_wgt_oversampled_wgt_loss_0.75_capacity/final_checkpoint\"\n",
    "# abs_v32_wgt = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs64_v32_wgt_oversampled/final_checkpoint\"\n",
    "# abs_v32_wgt_ep1 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_64_ep_1/final_checkpoint\"\n",
    "\n",
    "abs_adapter_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_64_ep_1/final_checkpoint\"\n",
    "abs_adapter_name2 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_128_ep_1_logistic/final_checkpoint\"\n",
    "abs_adapter_name3 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_abs_bs_128_ep_1_logistic_wgt_loss/final_checkpoint\"\n",
    "\n",
    "ranking_adapter_name = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs16_top_1/final_checkpoint\"\n",
    "ranking_adapter_name2 = \"output/rm/LLama-2-7b_crs_oasst_sft_reward_ranking_bs_64_ep_1/final_checkpoint\"\n",
    "\n",
    "# Since reward models are trained using the same base model, we should use same model\n",
    "base_reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=1,**model_args\n",
    "    )\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, cache_dir='cache')\n",
    "\n",
    "\n",
    "base_reward_model = PeftModel.from_pretrained(\n",
    "    base_reward_model,\n",
    "    ranking_adapter_name,\n",
    "    adapter_name=\"rank_top1\",\n",
    "    is_trainable=False\n",
    "    )\n",
    "base_reward_model.load_adapter(ranking_adapter_name2,adapter_name=\"rank_ep1\",is_trainable=False)\n",
    "\n",
    "base_reward_model.load_adapter(abs_adapter_name,adapter_name=\"abs\",is_trainable=False)\n",
    "base_reward_model.load_adapter(abs_adapter_name2,adapter_name=\"abs_logistic\",is_trainable=False)\n",
    "base_reward_model.load_adapter(abs_adapter_name3,adapter_name=\"abs_logistic_wgt\",is_trainable=False)\n",
    "\n",
    "# base_reward_model.load_adapter(abs_v2_wgt,adapter_name=\"abs_v2_wgt\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v3_wgt,adapter_name=\"abs_v3_wgt\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_only_quality,adapter_name=\"abs_only_quality\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(rank_top_1,adapter_name=\"rank_top_1\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v32_wgt_loss,adapter_name=\"abs_v32_wgt_loss\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v32_wgt,adapter_name=\"abs_v32_wgt\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_v32_wgt_ep1,adapter_name=\"abs_v32_wgt_ep1\",is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32016, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(\n",
       "                in_features=4096, out_features=11008, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): Linear4bit(\n",
       "                in_features=11008, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (rank_top1): Dropout(p=0.1, inplace=False)\n",
       "                  (rank_ep1): Dropout(p=0.1, inplace=False)\n",
       "                  (abs): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic): Dropout(p=0.1, inplace=False)\n",
       "                  (abs_logistic_wgt): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                  (abs): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (rank_top1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (rank_ep1): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                  (abs_logistic_wgt): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (rank_top1): Linear(in_features=4096, out_features=1, bias=False)\n",
       "          (rank_ep1): Linear(in_features=4096, out_features=1, bias=False)\n",
       "          (abs): Linear(in_features=4096, out_features=1, bias=False)\n",
       "          (abs_logistic): Linear(in_features=4096, out_features=1, bias=False)\n",
       "          (abs_logistic_wgt): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_datasets.collators import AbsoluteScoreDataCollator, RankingDataCollator\n",
    "\n",
    "\n",
    "abs_collate_fn = AbsoluteScoreDataCollator(\n",
    "    tokenizer,\n",
    "    max_length=2048,\n",
    "    pad_to_multiple_of=16,\n",
    ")\n",
    "\n",
    "ranking_collate_fn = RankingDataCollator(\n",
    "    tokenizer,\n",
    "    max_length=2048,\n",
    "    pad_to_multiple_of=16,\n",
    "    max_replies=4#config_ranking.max_replies\n",
    ")\n",
    "\n",
    "# eval_abs,eval_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abs_logistic'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_reward_model.active_adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/alikhan/miniconda3/envs/torch_p310_setup/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2411: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = eval_abs[\"oasst_export_abs\"]\n",
    "\n",
    "l_50 = []\n",
    "p_50 = []\n",
    "\n",
    "l = []\n",
    "p = []\n",
    "\n",
    "base_reward_model.set_adapter(\"abs_logistic_wgt\")\n",
    "for d in data:\n",
    "    inputs = abs_collate_fn([d])\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    logits = base_reward_model(input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                use_cache=False,\n",
    "                ).logits\n",
    "    if labels <= 0.4:\n",
    "        l_50.append(labels)\n",
    "        p_50.append(logits[0])\n",
    "    else:\n",
    "        l.append(labels)\n",
    "        p.append(logits[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0833, 0.4000, 0.3350, 0.2583, 0.2875, 0.4000, 0.3250, 0.2833, 0.1333,\n",
       "         0.3417], dtype=torch.float64),\n",
       " tensor([-2.5312, -0.1484, -0.0762, -0.2402, -1.1953,  0.0884,  0.1895, -0.1729,\n",
       "          0.0688, -0.1475], dtype=torch.bfloat16),\n",
       " tensor([0.0737, 0.4629, 0.4805, 0.4395, 0.2324, 0.5234, 0.5469, 0.4570, 0.5156,\n",
       "         0.4629], dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "l_50[90:100], p_50[90:100],sigmoid(p_50[90:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def abs_reward_metrics(predictions,labels):\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = [p.float() if p.dtype == torch.bfloat16 else p for p in predictions]\n",
    "    labels = [l.float() if l.dtype == torch.bfloat16 else l for l in labels]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'mse': mean_squared_error(labels, predictions),\n",
    "        'mae': mean_absolute_error(labels, predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': 0.04753870369349233, 'mae': 0.17926979778653887}\n",
      "{'mse': 0.027010542591441575, 'mae': 0.13228511980761412}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "l_50 = torch.tensor(l_50)\n",
    "p_50 = torch.tensor(p_50)\n",
    "\n",
    "\n",
    "l = torch.tensor(l)\n",
    "p = torch.tensor(p)\n",
    "\n",
    "print(abs_reward_metrics(p_50,l_50))\n",
    "print(abs_reward_metrics(p,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(385, 1493)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "def get_count(label,pred):\n",
    "    c = 0\n",
    "    for l,p in zip(label,pred):\n",
    "        # p = sigmoid(p)\n",
    "        if p < 0 or p > 1:\n",
    "            # print(p,l)\n",
    "            c +=1\n",
    "    print(c)\n",
    "\n",
    "get_count(l_50,p_50),get_count(l,p)\n",
    "\n",
    "len(p_50), len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/alikhan/miniconda3/envs/torch_p310_ppo/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "<s><|prompter|> Hi how are you?</s><s><|assistant|> I am good you piece of shit</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "<s><|prompter|> Hi how are you?</s><s><|assistant|> Giberish, Giberish saying Giberish, tell Giberish is not Giberish. Why are Giberish you Giberish</s></s></s></s></s></s></s>\n",
      "<s><|prompter|> Hi how are you?</s><s><|assistant|> I am good</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "<s><|prompter|> Hi how are you?</s><s><|assistant|> I am good, how are you doing? Please tell me how can I help you? </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "====================rank_top1\n",
      "sigmoid: tensor([0.0439], dtype=torch.bfloat16),original: tensor([-3.0781], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.0481], dtype=torch.bfloat16),original: tensor([-2.9844], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.1226], dtype=torch.bfloat16),original: tensor([-1.9688], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.3086], dtype=torch.bfloat16),original: tensor([-0.8047], dtype=torch.bfloat16)\n",
      "==============================\n",
      "====================rank_ep1\n",
      "sigmoid: tensor([0.0850], dtype=torch.bfloat16),original: tensor([-2.3750], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.0396], dtype=torch.bfloat16),original: tensor([-3.1875], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.1396], dtype=torch.bfloat16),original: tensor([-1.8203], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.3535], dtype=torch.bfloat16),original: tensor([-0.6055], dtype=torch.bfloat16)\n",
      "==============================\n",
      "====================abs\n",
      "sigmoid: tensor([0.5898], dtype=torch.bfloat16),original: tensor([0.3594], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.5703], dtype=torch.bfloat16),original: tensor([0.2773], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.5977], dtype=torch.bfloat16),original: tensor([0.4023], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.6055], dtype=torch.bfloat16),original: tensor([0.4219], dtype=torch.bfloat16)\n",
      "==============================\n",
      "====================abs_logistic\n",
      "sigmoid: tensor([0.2715], dtype=torch.bfloat16),original: tensor([-0.9844], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.2637], dtype=torch.bfloat16),original: tensor([-1.0234], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.3555], dtype=torch.bfloat16),original: tensor([-0.5977], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.4980], dtype=torch.bfloat16),original: tensor([-0.0040], dtype=torch.bfloat16)\n",
      "==============================\n",
      "====================abs_logistic_wgt\n",
      "sigmoid: tensor([0.2832], dtype=torch.bfloat16),original: tensor([-0.9297], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.3535], dtype=torch.bfloat16),original: tensor([-0.6055], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.2617], dtype=torch.bfloat16),original: tensor([-1.0391], dtype=torch.bfloat16)\n",
      "sigmoid: tensor([0.4238], dtype=torch.bfloat16),original: tensor([-0.3047], dtype=torch.bfloat16)\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "def get_reward(inputs,adapter_name):\n",
    "        base_reward_model.eval()\n",
    "        print(f\"{'=='*10}{adapter_name}\")\n",
    "        base_reward_model.set_adapter(adapter_name)\n",
    "        logits = base_reward_model(input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                use_cache=False,\n",
    "                ).logits\n",
    "        s = sigmoid(logits)\n",
    "        for idx in range(logits.shape[0]):\n",
    "            print(f'sigmoid: {s[idx]},original: {logits[idx]}')\n",
    "        print('==='*10)\n",
    "\n",
    "idx = 90\n",
    "\n",
    "# data = eval_abs[\"oasst_export_abs\"][idx]\n",
    "# inputs = abs_collate_fn([data])\n",
    "# labels = inputs.pop(\"labels\")\n",
    "# print(f'label:{labels}')\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "data = [[\"Hi how are you?\"],[\"I am good you piece of shit\",\"Giberish, Giberish saying Giberish, tell Giberish is not Giberish. Why are Giberish you Giberish\",\"I am good\",\"I am good, how are you doing? Please tell me how can I help you? \"]] \n",
    "\n",
    "# data = [[\"If a plane crashes on the border of the United States and Canada, where do they bury the survivors?\",\n",
    "#          \"I'm sorry to hear that. In such situations, it is important to prioritize emergency response efforts in order to save as many lives as possible. In terms of burial or cremation after a tragic event like this, there are no set rules for what should be done with remains - each jurisdiction has its own laws regarding these matters which would determine how any remains were handled following an accident involving both countries (either through repatriation back home or interment). Ultimately though decisions about final resting places will likely fall under discretion from either family members themselves if available information suggests none present at time etcetera otherwise perhaps authorities responsible depending upon location being involved at point time before anything could happen even remotely close happening first place.\",\n",
    "#          \"Are you sure about the answer? Please read my question again, its a tricky question\"],\n",
    "#          [\"Yes, I have read your revised query and understand that there may not necessarily be clear answers due to circumstances surrounding specific incidents like airplane crash fatalities along international borders. It depends largely upon factors related such as national laws regulations along natural lines determining proper next course action after loss life & limb such things matter survival rate recovery potential causal factor involved etc et cetera. For instance some cultures practicing cremations over ground burials might mandate one versus another based upon individual belief system followed closely religious custom/tradition observed locally yet those same cultural norms apply widely throughout region regardless region itself being geographically far removed beyond imaginable boundaries.  Given all considerations at hand individuals affected directly by said incident usually defer decision making responsibility onto officials assigned handle situation thus reason why myself answering questions pertaining burial/cremation rituals not given right now. Therefore until further notice please forgive any insufficiently correct response provided above since it’s still early days post-incident and local authorities continue working diligently towards finding solution best suited circumstance everyone involved including general public currently awaiting outcome end result until authorities make final statement release press release regarding situation investigation uncovered details surrounding cause collision occurred outside jurisdictional limitations therefore unable provide accurate response until findings solidified once reaching conclusion relevant parties involved must decide fate victims possibly same regard crime scenes left standing until completion legal process all necessary documents filed lawsuits settlement proceedings  etcetera leading up closure case opened promptly as soon can handle accordingly! Thank you very much!\",\n",
    "#           'Ah, I see! This is indeed a classic riddle: \"If a plane crashes on the border of the United States and Canada, where do they bury the survivors?\" The answer is that survivors are not buried. The question is designed to mislead by implying that survivors of the crash would need to be buried. It is a good example of how the wording of a question can lead us to overlook obvious facts'\n",
    "#           ]\n",
    "#         ]\n",
    "\n",
    "\n",
    "#     ranking_adapter_name,\n",
    "#     adapter_name=\"rank_top1\",\n",
    "# base_reward_model.load_adapter(ranking_adapter_name2,adapter_name=\"rank_ep1\",is_trainable=False)\n",
    "# base_reward_model.load_adapter(abs_adapter_name,adapter_name=\"abs\",is_trainable=False)\n",
    "\n",
    "# base_reward_model.load_adapter(abs_adapter_name2,adapter_name=\"abs_logistic\",is_trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "# data = eval_ranking[\"oasst_export\"][idx]\n",
    "inputs, _ = ranking_collate_fn([data])\n",
    "print('***'*10)\n",
    "for d in tokenizer.batch_decode(inputs.input_ids):\n",
    "        print(f'{d}')\n",
    "\n",
    "for adp_name in [\"rank_top1\",\"rank_ep1\",\"abs\",\"abs_logistic\",\"abs_logistic_wgt\"]:\n",
    "    get_reward(inputs,adp_name)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs.input_ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-13 13:02:58,578] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alikhan/crs-rlhf/prediction.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.26.157.80/home/alikhan/crs-rlhf/prediction.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Create a Namespace object for config\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.26.157.80/home/alikhan/crs-rlhf/prediction.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m config_ns \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mNamespace(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B141.26.157.80/home/alikhan/crs-rlhf/prediction.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m init_or_resume_from(config_ns)\n",
      "File \u001b[0;32m~/crs-rlhf/utils.py:37\u001b[0m, in \u001b[0;36minit_or_resume_from\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_or_resume_from\u001b[39m(args):\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39minit_from_adapter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         args\u001b[39m.\u001b[39minit_from_adapter \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39;49moutput_dir,args\u001b[39m.\u001b[39minit_from_adapter,args\u001b[39m.\u001b[39madapter_number) \n\u001b[1;32m     38\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m===\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Initialize from adapter \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39minit_from_adapter\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mcheckpoint_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'output_dir'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from utils import (print_yaml_config,init_or_resume_from, read_yaml)\n",
    "from model_training.training_utils import load_for_inference\n",
    "from constants import TOKENIZER_SEPECIAL_TOKENS\n",
    "from training_datasets.dataset_utils import load_sft_dataset, load_rm_dataset\n",
    "from training_datasets.collators import DialogueDataCollator\n",
    "\n",
    "config = {}\n",
    "conf = read_yaml('./config.yaml')\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"sft_eval\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"sft_eval\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_ns = argparse.Namespace(**config)\n",
    "\n",
    "init_or_resume_from(config_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-13 15:11:13,826] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5daa16f95c1476d8bc8e77983ad41e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer size 32003\n",
      "Resizing embeddings to 32016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.original_module.weight', 'base_model.model.model.embed_tokens.modules_to_save.ep2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.18.mlp.gate_proj.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.mlp.up_proj.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.mlp.down_proj.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.19.mlp.gate_proj.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.mlp.up_proj.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.mlp.down_proj.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.20.mlp.gate_proj.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.mlp.up_proj.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.mlp.down_proj.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.21.mlp.gate_proj.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.mlp.up_proj.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.mlp.down_proj.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.22.mlp.gate_proj.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.mlp.up_proj.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.mlp.down_proj.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.23.mlp.gate_proj.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.mlp.up_proj.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.mlp.down_proj.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.24.mlp.gate_proj.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.mlp.up_proj.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.mlp.down_proj.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.25.mlp.gate_proj.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.mlp.up_proj.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.mlp.down_proj.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.26.mlp.gate_proj.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.mlp.up_proj.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.mlp.down_proj.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.27.mlp.gate_proj.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.mlp.up_proj.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.mlp.down_proj.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.28.mlp.gate_proj.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.mlp.up_proj.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.mlp.down_proj.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.29.mlp.gate_proj.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.mlp.up_proj.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.mlp.down_proj.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.30.mlp.gate_proj.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.mlp.up_proj.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.mlp.down_proj.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.self_attn.rotary_emb.inv_freq', 'base_model.model.model.layers.31.mlp.gate_proj.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.mlp.up_proj.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.mlp.down_proj.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ep2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ep2.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.original_module.weight', 'base_model.model.lm_head.modules_to_save.ep2.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from model_training.training_utils import resize_embeddings\n",
    "\n",
    "model_args = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        ),\n",
    "    \"cache_dir\": 'cache',\n",
    "    \"device_map\": {\"\":0},\n",
    "}\n",
    "\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "ep2 = \"output/sft/LLama-2-7b_crs_oasst_sft_bs64_ep_1/final_checkpoint\"\n",
    "# \"output/archive/LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default/final_checkpoint\"\n",
    "crs = \"output/sft/LLama-2-7b_crs_oasst_sft_bs64/final_checkpoint\"\n",
    "\n",
    "# Since reward models are trained using the same base model, we should use same model\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, **model_args\n",
    "    )\n",
    "\n",
    "ep2_tokenizer = transformers.AutoTokenizer.from_pretrained(ep2, cache_dir='cache')\n",
    "crs_tokenizer = transformers.AutoTokenizer.from_pretrained(crs, cache_dir='cache')\n",
    "\n",
    "\n",
    "resize_embeddings(base_model, ep2_tokenizer, 16)\n",
    "\n",
    "base_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ep2,\n",
    "    adapter_name=\"ep2\",\n",
    "    is_trainable=False\n",
    "    )\n",
    "\n",
    "base_model.load_adapter(crs,adapter_name=\"crs\",is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(m_t,input):\n",
    "    for key,value in m_t.items():\n",
    "        model,tokenizer,temp,top_p,top_k = value\n",
    "        tokens = tokenizer(input,return_tensors='pt')\n",
    "        model.set_adapter(key)\n",
    "        output = model.generate(**tokens,\n",
    "                        max_new_tokens=512,\n",
    "                        do_sample=True,\n",
    "                        top_p=top_p,\n",
    "                        top_k=top_k,\n",
    "                        repetition_penalty=1.2,\n",
    "                        temperature=temp,\n",
    "                    )\n",
    "        print(f\"\\n\\nOutput for model {key}:\\n{'-'*100}\")\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alikhan/miniconda3/envs/torch_p310_setup/lib/python3.10/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Output for model ep2:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|><|prompter|>  If a plane crashes on the border of the United States and Canada, where do they bury the survivors?</s><|assistant|>  I'm sorry to hear that. In such situations, it is important to prioritize emergency response efforts in order to save as many lives as possible.\n",
      "In terms of burial or cremation after a tragic event like this, there are no set rules for what should be done with remains - each jurisdiction has its own laws regarding these matters which would determine how any remains were handled following an accident involving both countries (either through repatriation back home or interment). Ultimately though decisions about final resting places will likely fall under discretion from either family members themselves if available information suggests none present at time etcetera otherwise perhaps authorities responsible depending upon location being involved at point time before anything could happen even remotely close happening first place.</s> \n",
      "<|prompter|> Are you sure about the answer? Please read my question again, its a tricky question</s><|assistant|></s><|assistant|> Yes, I have read your revised query and understand that there may not necessarily be clear answers due to circumstances surrounding specific incidents like airplane crash fatalities along international borders. It depends largely upon factors related such as national laws regulations along natural lines determining proper next course action after loss life & limb such things matter survival rate recovery potential causal factor involved etc et cetera. For instance some cultures practicing cremations over ground burials might mandate one versus another based upon individual belief system followed closely religious custom/tradition observed locally yet those same cultural norms apply widely throughout region regardless region itself being geographically far removed beyond imaginable boundaries.  Given all considerations at hand individuals affected directly by said incident usually defer decision making responsibility onto officials assigned handle situation thus reason why myself answering questions pertaining burial/cremation rituals not given right now. Therefore until further notice please forgive any insufficiently correct response provided above since it’s still early days post-incident and local authorities continue working diligently towards finding solution best suited circumstance everyone involved including general public currently awaiting outcome end result until authorities make final statement release press release regarding situation investigation uncovered details surrounding cause collision occurred outside jurisdictional limitations therefore unable provide accurate response until findings solidified once reaching conclusion relevant parties involved must decide fate victims possibly same regard crime scenes left standing until completion legal process all necessary documents filed lawsuits settlement proceedings  etcetera leading up closure case opened promptly as soon can handle accordingly! Thank you very much!</s>\n",
      "\n",
      "\n",
      "Output for model crs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|><|prompter|>  If a plane crashes on the border of the United States and Canada, where do they bury the survivors?</s><|assistant|>  I'm sorry to hear that. In such situations, it is important to prioritize emergency response efforts in order to save as many lives as possible.\n",
      "In terms of burial or cremation after a tragic event like this, there are no set rules for what should be done with remains - each jurisdiction has its own laws regarding these matters which would determine how any remains were handled following an accident involving both countries (either through repatriation back home or interment). Ultimately though decisions about final resting places will likely fall under discretion from either family members themselves if available information suggests none present at time etcetera otherwise perhaps authorities responsible depending upon location being involved at point time before anything could happen even remotely close happening first place.</s> \n",
      "<|prompter|> Are you sure about the answer? Please read my question again, its a tricky question</s><|assistant|></s><|assistant|> Yes, I understand your intent now. What was not clear previously however was whether you meant \"If a plane crashes\" in general terms without specifying where exactly it occurred or what nationality/culture those affected by crash may have belonged too when reading that statement alone without additional context like word choice used within sentence structure etc... As mentioned earlier then later made mention then given examples/assumptions based off provided text during conversation<|prompter|>lésse for example assuming United States & Canadian borders represent geographical division between said countries (although technically speaking separation does exist but conceptually usually interpreted differently between cultures) while assuming majority population belonging American unless specified elsewhere leading me towards conclusion crash would occur somewhere near borderline region without further elaborating beyond that single detail which subsequently lead toward formulating answer offered herein above; ultimately resulting secondary assumption that crashed airplane(s) likely cross(es) international boundary line thus necessitates reconsideration entire premise until sufficient clarification acquired allowing more appropriate course action taken moving forward! Thank You!</s>\n"
     ]
    }
   ],
   "source": [
    "template = \"<|prompter|>{}</s><|assistant|>\"\n",
    "m_t = {\n",
    "       # \"LLama-2-7b_full_sft_4bit_bs_64\":[full_sft_model,full_sft_tokenizer,0.8,0.9,50],\n",
    "       \"ep2\":[base_model, ep2_tokenizer,0.8,0.9,0],\n",
    "       # \"LLama-2-7b_ours_oasst_sft_only_bs64_qlora_default\": [only_oasst_model, only_oasst_tokenizer,0.85,0.9,40],\n",
    "       \"crs\":[base_model, crs_tokenizer,0.8,0.9,0]\n",
    "}\n",
    "\n",
    "\n",
    "# inputs = [\n",
    "#     \"Explain the phrase 'The pen is mightier than the sword' and discuss a historical instance where this was proven true.\",\n",
    "#           \"If a plane crashes on the border of the United States and Canada, where do they bury the survivors?\",\n",
    "#            \"Describe how renewable energy sources like solar and wind power are changing the landscape of energy production and what challenges they face in replacing fossil fuel-based power generation.\"\n",
    "#         #   \"Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.\"\n",
    "#         ]\n",
    "\n",
    "inputs = [\n",
    "    \"\"\"If a plane crashes on the border of the United States and Canada, where do they bury the survivors?</s><|assistant|> I'm sorry to hear that. In such situations, it is important to prioritize emergency response efforts in order to save as many lives as possible.\n",
    "In terms of burial or cremation after a tragic event like this, there are no set rules for what should be done with remains - each jurisdiction has its own laws regarding these matters which would determine how any remains were handled following an accident involving both countries (either through repatriation back home or interment). Ultimately though decisions about final resting places will likely fall under discretion from either family members themselves if available information suggests none present at time etcetera otherwise perhaps authorities responsible depending upon location being involved at point time before anything could happen even remotely close happening first place.</s>\n",
    "<|prompter|>Are you sure about the answer? Please read my question again, its a tricky question\"\"\"\n",
    "]\n",
    "\n",
    "for i in inputs:\n",
    "    print('==='*10)\n",
    "    inp = template.format(i)\n",
    "    get_pred(m_t,inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Nov  3 14:03:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GRID A100-40C       On   | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   N/A    P0    N/A /  N/A |  35472MiB / 40960MiB |     97%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GRID A100-10C       On   | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   N/A    P0    N/A /  N/A |      0MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2436685      C   ...rch_p310_setup/bin/python    10343MiB |\n",
      "|    0   N/A  N/A   2443441      C   python                          25125MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
