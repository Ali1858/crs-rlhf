{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alikhan/miniconda3/envs/torch_p310_setup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Initialize from adapter output/LLama-2-7b_pre_sft_warm_20/final_checkpoint\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from utils import (print_yaml_config,init_or_resume_from, read_yaml)\n",
    "from model_training.training_utils import load_for_inference\n",
    "from constants import TOKENIZER_SEPECIAL_TOKENS\n",
    "from training_datasets.dataset_utils import load_sft_dataset, load_rm_dataset\n",
    "from training_datasets.collators import DialogueDataCollator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(conf):\n",
    "    print(f\"\\n{'==='*10} Following are the configuration for training{'==='*10}\")\n",
    "    print_yaml_config(conf)\n",
    "    device_map = {\"\":0}\n",
    "    special_tokens = TOKENIZER_SEPECIAL_TOKENS[\"llama\"]\n",
    "    model, tokenizer = load_for_inference(device_map,conf,special_tokens,conf.model_name,False)\n",
    "    return model, tokenizer\n",
    "\n",
    "config = {}\n",
    "conf = read_yaml('./config.yaml')\n",
    "config.update(conf[\"default\"])\n",
    "config.update(conf[\"sft_eval\"])\n",
    "config[\"name_suffix\"] = \"\"\n",
    "config[\"debug\"] = False\n",
    "config[\"subset\"] = \"sft_eval\"\n",
    "\n",
    "# Create a Namespace object for config\n",
    "config_ns = argparse.Namespace(**config)\n",
    "\n",
    "init_or_resume_from(config_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Initialize from adapter output/LLama-2-7b_ours_oasst_sftonly_bs64_qlora_default/final_checkpoint\n",
      "\n",
      "============================== Following are the configuration for training==============================\n",
      "!!python/object:argparse.Namespace\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_epsilon: 1e-12\n",
      "adapter_number: final_checkpoint\n",
      "adpater_name: null\n",
      "checkpoint_name: null\n",
      "checkpoint_number: final_checkpoint\n",
      "collator:\n",
      "  label_masking: true\n",
      "  max_length: 2048\n",
      "  random_offset_probability: 0.5\n",
      "  samples_mixing: false\n",
      "  system_prefix: null\n",
      "  use_system_prefix: false\n",
      "dataset:\n",
      "  alpaca:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  dolly:\n",
      "    max_val_set: 300\n",
      "    val_split: 0.05\n",
      "  math_instruction:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  oasst_export:\n",
      "    lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\n",
      "    max_val_set: null\n",
      "    val_split: 0.05\n",
      "  vicuna:\n",
      "    max_val_set: 800\n",
      "    val_split: 0.05\n",
      "  webgpt:\n",
      "    max_val_set: 1000\n",
      "    val_split: 0.05\n",
      "debug: false\n",
      "debug_set: 100\n",
      "dtype: bf16\n",
      "early_stopping: false\n",
      "eval_accumulation_steps: null\n",
      "eval_batch: 4\n",
      "eval_steps: 100\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: true\n",
      "hpt_data_frac: null\n",
      "init_from_adapter: output/LLama-2-7b_ours_oasst_sftonly_bs64_qlora_default/final_checkpoint\n",
      "int4_training: true\n",
      "int8_training: false\n",
      "is_abs_rm: null\n",
      "log_steps: 10\n",
      "lr: 1e-4\n",
      "lr_scheduler_type: cosine\n",
      "max_grad_norm: 0.3\n",
      "max_steps: 0\n",
      "merged_adapter_path: null\n",
      "model_name: meta-llama/Llama-2-7b-hf\n",
      "name: LLama-2-7b_eval\n",
      "name_suffix: ''\n",
      "num_train_epochs: 1\n",
      "oasst_sft: LLama-2-7b_ours_oasst_sft\n",
      "output_dir: output\n",
      "pre_sft: LLama-2-7b_ours_oasst_sft\n",
      "ranking_rm: LLama-2-7b_oasst_sft_ranking_reward\n",
      "report_to: wandb\n",
      "resume_from_checkpoint: null\n",
      "save_steps: 100\n",
      "subset: sft_eval\n",
      "train_batch: 4\n",
      "warmup_ratio: 0.03\n",
      "weight_decay: 0.0\n",
      "\n",
      "loading meta-llama/Llama-2-7b-hf model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer size 32003\n",
      "Resizing embeddings to 32016\n"
     ]
    }
   ],
   "source": [
    "config_ns.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "config_ns.init_from_adapter = \"LLama-2-7b_ours_oasst_sftonly_bs64_qlora_default\"\n",
    "init_or_resume_from(config_ns)\n",
    "only_oasst_model, only_oasst_tokenizer = main(config_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Initialize from adapter output/LLama-2-7b_full_sft_4bit_bs_64/final_checkpoint\n",
      "\n",
      "============================== Following are the configuration for training==============================\n",
      "!!python/object:argparse.Namespace\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_epsilon: 1e-12\n",
      "adapter_number: final_checkpoint\n",
      "adpater_name: null\n",
      "checkpoint_name: null\n",
      "checkpoint_number: final_checkpoint\n",
      "collator:\n",
      "  label_masking: true\n",
      "  max_length: 2048\n",
      "  random_offset_probability: 0.5\n",
      "  samples_mixing: false\n",
      "  system_prefix: null\n",
      "  use_system_prefix: false\n",
      "dataset:\n",
      "  alpaca:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  dolly:\n",
      "    max_val_set: 300\n",
      "    val_split: 0.05\n",
      "  math_instruction:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  oasst_export:\n",
      "    lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\n",
      "    max_val_set: null\n",
      "    val_split: 0.05\n",
      "  vicuna:\n",
      "    max_val_set: 800\n",
      "    val_split: 0.05\n",
      "  webgpt:\n",
      "    max_val_set: 1000\n",
      "    val_split: 0.05\n",
      "debug: false\n",
      "debug_set: 100\n",
      "dtype: bf16\n",
      "early_stopping: false\n",
      "eval_accumulation_steps: null\n",
      "eval_batch: 4\n",
      "eval_steps: 100\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: true\n",
      "hpt_data_frac: null\n",
      "init_from_adapter: output/LLama-2-7b_full_sft_4bit_bs_64/final_checkpoint\n",
      "int4_training: true\n",
      "int8_training: false\n",
      "is_abs_rm: null\n",
      "log_steps: 10\n",
      "lr: 1e-4\n",
      "lr_scheduler_type: cosine\n",
      "max_grad_norm: 0.3\n",
      "max_steps: 0\n",
      "merged_adapter_path: null\n",
      "model_name: meta-llama/Llama-2-7b-hf\n",
      "name: LLama-2-7b_eval\n",
      "name_suffix: ''\n",
      "num_train_epochs: 1\n",
      "oasst_sft: LLama-2-7b_ours_oasst_sft\n",
      "output_dir: output\n",
      "pre_sft: LLama-2-7b_ours_oasst_sft\n",
      "ranking_rm: LLama-2-7b_oasst_sft_ranking_reward\n",
      "report_to: wandb\n",
      "resume_from_checkpoint: null\n",
      "save_steps: 100\n",
      "subset: sft_eval\n",
      "train_batch: 4\n",
      "warmup_ratio: 0.03\n",
      "weight_decay: 0.0\n",
      "\n",
      "loading meta-llama/Llama-2-7b-hf model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer size 32003\n",
      "Resizing embeddings to 32016\n"
     ]
    }
   ],
   "source": [
    "config_ns.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "config_ns.init_from_adapter = \"LLama-2-7b_full_sft_4bit_bs_64\"\n",
    "init_or_resume_from(config_ns)\n",
    "full_sft_model, full_sft_tokenizer = main(config_ns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Initialize from adapter output/LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default/final_checkpoint\n",
      "\n",
      "============================== Following are the configuration for training==============================\n",
      "!!python/object:argparse.Namespace\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_epsilon: 1e-12\n",
      "adapter_number: final_checkpoint\n",
      "adpater_name: null\n",
      "checkpoint_name: null\n",
      "checkpoint_number: final_checkpoint\n",
      "collator:\n",
      "  label_masking: true\n",
      "  max_length: 2048\n",
      "  random_offset_probability: 0.5\n",
      "  samples_mixing: false\n",
      "  system_prefix: null\n",
      "  use_system_prefix: false\n",
      "dataset:\n",
      "  alpaca:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  dolly:\n",
      "    max_val_set: 300\n",
      "    val_split: 0.05\n",
      "  math_instruction:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  oasst_export:\n",
      "    lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\n",
      "    max_val_set: null\n",
      "    val_split: 0.05\n",
      "  vicuna:\n",
      "    max_val_set: 800\n",
      "    val_split: 0.05\n",
      "  webgpt:\n",
      "    max_val_set: 1000\n",
      "    val_split: 0.05\n",
      "debug: false\n",
      "debug_set: 100\n",
      "dtype: bf16\n",
      "early_stopping: false\n",
      "eval_accumulation_steps: null\n",
      "eval_batch: 4\n",
      "eval_steps: 100\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: true\n",
      "hpt_data_frac: null\n",
      "init_from_adapter: output/LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default/final_checkpoint\n",
      "int4_training: true\n",
      "int8_training: false\n",
      "is_abs_rm: null\n",
      "log_steps: 10\n",
      "lr: 1e-4\n",
      "lr_scheduler_type: cosine\n",
      "max_grad_norm: 0.3\n",
      "max_steps: 0\n",
      "merged_adapter_path: null\n",
      "model_name: meta-llama/Llama-2-7b-hf\n",
      "name: LLama-2-7b_eval\n",
      "name_suffix: ''\n",
      "num_train_epochs: 1\n",
      "oasst_sft: LLama-2-7b_ours_oasst_sft\n",
      "output_dir: output\n",
      "pre_sft: LLama-2-7b_ours_oasst_sft\n",
      "ranking_rm: LLama-2-7b_oasst_sft_ranking_reward\n",
      "report_to: wandb\n",
      "resume_from_checkpoint: null\n",
      "save_steps: 100\n",
      "subset: sft_eval\n",
      "train_batch: 4\n",
      "warmup_ratio: 0.03\n",
      "weight_decay: 0.0\n",
      "\n",
      "loading meta-llama/Llama-2-7b-hf model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer size 32003\n",
      "Resizing embeddings to 32016\n"
     ]
    }
   ],
   "source": [
    "config_ns.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "config_ns.init_from_adapter = \"LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default\"\n",
    "config_ns.adapter_number= \"final_checkpoint\"\n",
    "init_or_resume_from(config_ns)\n",
    "ep2_qlora_sft_model, ep2_qlora_sft_tokenizer = main(config_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Initialize from adapter output/LLama-2-7b_ours_oasst_sft_ep_3_lr_1e4_cosine_bs64_qlora_default/final_checkpoint\n",
      "\n",
      "============================== Following are the configuration for training==============================\n",
      "!!python/object:argparse.Namespace\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.999\n",
      "adam_epsilon: 1e-12\n",
      "adapter_number: final_checkpoint\n",
      "adpater_name: null\n",
      "checkpoint_name: null\n",
      "checkpoint_number: final_checkpoint\n",
      "collator:\n",
      "  label_masking: true\n",
      "  max_length: 2048\n",
      "  random_offset_probability: 0.5\n",
      "  samples_mixing: false\n",
      "  system_prefix: null\n",
      "  use_system_prefix: false\n",
      "dataset:\n",
      "  alpaca:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  dolly:\n",
      "    max_val_set: 300\n",
      "    val_split: 0.05\n",
      "  math_instruction:\n",
      "    max_val_set: 200\n",
      "    val_split: 0.05\n",
      "  oasst_export:\n",
      "    lang: en,bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\n",
      "    max_val_set: null\n",
      "    val_split: 0.05\n",
      "  vicuna:\n",
      "    max_val_set: 800\n",
      "    val_split: 0.05\n",
      "  webgpt:\n",
      "    max_val_set: 1000\n",
      "    val_split: 0.05\n",
      "debug: false\n",
      "debug_set: 100\n",
      "dtype: bf16\n",
      "early_stopping: false\n",
      "eval_accumulation_steps: null\n",
      "eval_batch: 4\n",
      "eval_steps: 100\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: true\n",
      "hpt_data_frac: null\n",
      "init_from_adapter: output/LLama-2-7b_ours_oasst_sft_ep_3_lr_1e4_cosine_bs64_qlora_default/final_checkpoint\n",
      "int4_training: true\n",
      "int8_training: false\n",
      "is_abs_rm: null\n",
      "log_steps: 10\n",
      "lr: 1e-4\n",
      "lr_scheduler_type: cosine\n",
      "max_grad_norm: 0.3\n",
      "max_steps: 0\n",
      "merged_adapter_path: null\n",
      "model_name: meta-llama/Llama-2-7b-hf\n",
      "name: LLama-2-7b_eval\n",
      "name_suffix: ''\n",
      "num_train_epochs: 1\n",
      "oasst_sft: LLama-2-7b_ours_oasst_sft\n",
      "output_dir: output\n",
      "pre_sft: LLama-2-7b_ours_oasst_sft\n",
      "ranking_rm: LLama-2-7b_oasst_sft_ranking_reward\n",
      "report_to: wandb\n",
      "resume_from_checkpoint: null\n",
      "save_steps: 100\n",
      "subset: sft_eval\n",
      "train_batch: 4\n",
      "warmup_ratio: 0.03\n",
      "weight_decay: 0.0\n",
      "\n",
      "loading meta-llama/Llama-2-7b-hf model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer size 32003\n",
      "Resizing embeddings to 32016\n"
     ]
    }
   ],
   "source": [
    "config_ns.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "config_ns.init_from_adapter = \"LLama-2-7b_ours_oasst_sft_ep_3_lr_1e4_cosine_bs64_qlora_default\"\n",
    "config_ns.adapter_number= \"final_checkpoint\"\n",
    "init_or_resume_from(config_ns)\n",
    "ep3_qlora_sft_model, ep3_qlora_sft_tokenizer = main(config_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_t = {\n",
    "       \"LLama-2-7b_full_sft_4bit_bs_64\":[full_sft_model,full_sft_tokenizer,0.8,0.9,50],\n",
    "       \"LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default\":[ep2_qlora_sft_model, ep2_qlora_sft_tokenizer,0.8,0.9,40],\n",
    "       \"LLama-2-7b_ours_oasst_sft_only_bs64_qlora_default\": [only_oasst_model, only_oasst_tokenizer,0.85,0.9,40],\n",
    "       \"LLama-2-7b_ours_oasst_sft_ep_3_lr_1e4_cosine_bs64_qlora_default\":[ep3_qlora_sft_model, ep3_qlora_sft_tokenizer,0.8,0.9,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(m_t,input):\n",
    "    for key,value in m_t.items():\n",
    "        model,tokenizer,temp,top_p,top_k = value\n",
    "        tokens = tokenizer(input,return_tensors='pt')\n",
    "        output = model.generate(**tokens,\n",
    "                        max_new_tokens=512,\n",
    "                        do_sample=True,\n",
    "                        top_p=top_p,\n",
    "                        top_k=top_k,\n",
    "                        repetition_penalty=1.2,\n",
    "                        temperature=temp,\n",
    "                    )\n",
    "        print(f\"\\n\\nOutput for model {key}:\\n{'-'*100}\")\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Output for model LLama-2-7b_full_sft_4bit_bs_64:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.</s><|assistant|> With the advent of quantum computers and their ability to break certain types of cryptographic algorithms used in traditional security measures such as RSA and elliptic curve, it is important that existing systems are updated with newer, more secure ones which can be protected against these attacks.\n",
      "The main challenge faced by organizations will likely include creating a balance between providing adequate levels of protection while also keeping up with rapidly changing technologies.  Additionally, there may be concerns over whether any changes made now would be effective enough should future generations become available; however these issues remain unresolved at time of writing (June/2019). The potential benefits of embracing this technology include increased efficiency when dealing with large amounts of encrypted information due to faster computation times achieved through parallel processing capabilities offered by quantum computers compared to conventional CPUs. Furthermore, some argue that since quantum key distribution does not rely on secrecy but rather trust between two parties sharing keys across great distances then it could serve as an alternative form of authentication method whereby users' identity remains hidden even if compromised during transmission - making it much harder for attackers who have no access point nor knowledge about participants involved in said exchanges. However again further research must take place before conclusions can be drawn from either side so far regarding true efficacy levels under real-life conditions etc...</s>\n",
      "\n",
      "\n",
      "Output for model LLama-2-7b_ours_oasst_sft_ep_2_lr_1e4_cosine_bs64_qlora_default:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.</s><|assistant|> Quantum computers have the ability to quickly solve certain types of problems that are currently impossible for even the most powerful classical supercomputers. This has important consequences for cryptography: some widely used encryption algorithms (e.g., RSA) rely on mathematical assumptions that can be broken by a large enough quantum computer.\n",
      "\n",
      "One possible response is to modify existing encryption standards so they cannot easily be broken with a quantum computer. For example, there are proposed variations of the RSA algorithm that would still work against an attacker using only classical hardware; these modifications require a change from one part of the standard and are not yet implemented widespread. The other option is to replace traditional encryption altogether with post-quantum cryptographic techniques like lattice cryptography which provide much stronger security than their classical counterparts while remaining secure against all conceivable future threats including quantum computers.\n",
      "\n",
      "While we do not know exactly when someone will build a practical quantum computer capable of breaking today's best encryption standards, it may become feasible within 10–30 years if progress continues at roughly similar rates as seen recently. As such, experts recommend starting preparations now before any unforeseen event occurs later down the road — something already done by many organizations worldwide due to increasingly sophisticated attacks being carried out every day against sensitive systems belonging both public institutions (such as governments or banks) but also private companies storing confidential information about customers/employees alike.</s>\n",
      "\n",
      "\n",
      "Output for model LLama-2-7b_ours_oasst_sft_only_bs64_qlora_default:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.</s><|assistant|> Quantum computing is an emerging technology that has the potential to revolutionize several industries, including cryptography, security, and communications. The advent of quantum computers raises significant concerns about traditional encryption algorithms used today because they can be broken using Shor's algorithm which is a polynomial-time algorithm solving integer factorization problem. This means that any encryption method relying on RSA key size will eventually become vulnerable if large enough quantum computers are built.\n",
      "\n",
      "To address these risks, researchers have developed post-quantum cryptographic systems such as lattice-based cryptosystems like New Hope, code-based cryptosystems like McEliece, multilinear maps based cryptosystems (e.g., XMSS) etcetera. These alternatives offer better resistance against quantum computers but are not without flaws either - most recently, some attacks against those cryptographic schemes were discovered by researchers from Ruhr University Bochum in Germany. Therefore it remains important to monitor developments closely both at national level where governments invest heavily into protective technologies as well internationally where collaboration between nations could help find best solutions faster than individuals working alone would achieve given time constraints imposed upon them due lack resources required ect... It is clear however; there needs improvements made now before we get ourselves stuck in situation where everything goes haywire all over place just because people did nothing while opportunity presented itself before their eyes!</s>\n",
      "\n",
      "\n",
      "Output for model LLama-2-7b_ours_oasst_sft_ep_3_lr_1e4_cosine_bs64_qlora_default:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<s><|prompter|> Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.</s><|assistant|> Quantum computers have been shown to be capable of breaking some of the most robust cryptographic algorithms currently used in practice with relatively modest amounts of computation time. As such, it is important that we take steps now to ensure our systems are designed with these threats in mind.\n",
      "\n",
      "While there has already been substantial work done on developing post-quantum cryptography - a set of techniques aimed at providing strong protection against attacks by future generations of quantum computers - much remains unknown about exactly which types of attack will become feasible as progress continues towards building large-scale fault-tolerant quantum devices. Therefore, even if today's best practices can continue to secure us from known quantum threat scenarios, they may not remain adequate once more advanced technologies become available (though this scenario still lies far into an uncertain future).\n",
      "\n",
      "Aside from technical issues related specifically to security protocol design itself though; larger questions around human rights abuses & surveillance also arise when considering how trustworthy our information infrastructure really is. With government agencies worldwide actively investing huge resources into acquiring capabilities similar those possessed only recently by tech giants like Apple Inc., Google Inc., Microsoft Corporation among others – including access points directly onto private networks belonging individuals – it becomes clear why many people feel uneasy when discussing topics like \"information costa\" without having assurance over where said cost ends up being spent! In short: We should always keep asking ourselves whether all parties involved truly deserve their piece of pie? Does one party gain unfair advantage over another through nefarious means? How do we prevent bad actors exploiting vulnerabilities within existing frameworks? Only then can we hope achieve true harmony between individuals while ensuring long term stability within any given system. \n",
      "\n",
      "And so begins yet another cycle beginning again...\n",
      "\n",
      "What's next? Who knows...but here's hoping whatever comes next doesn't break down anymore walls than were torn apart before!</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = \"\"\"<|prompter|>Considering the advancements in quantum computing and its potential implications for classical encryption methods, explain how current cybersecurity measures might need to adapt, and what challenges or benefits this new computational paradigm could present to the fields of data privacy and digital communication.</s><|assistant|>\"\"\"\n",
    "get_pred(m_t,input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p310_setup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
